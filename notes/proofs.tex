\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{mathtools}
\usepackage{csquotes}
\usepackage[pdfborder={0 0 0}]{hyperref}

\newcommand\E{\mathbb{E}}
\newcommand\N{\mathcal{N}}
\DeclareMathOperator\cov{cov}
\DeclareMathOperator\var{var}

\newcommand\given{
  \nonscript\:
  \vert
  \nonscript\:
  \mathopen{}
  \allowbreak
}

\title{Mathematical notes on Bishop's book on Pattern Recognition and Machine Learning}
\author{Lukas Prokop}
\date{October 2016 to February 2016}

\begin{document}
\maketitle
\tableofcontents
\clearpage

\section{Preliminaries}

\section{Exercise 1.5}

\begin{description}
  \item[Given] $\var[f] = \E[(f(x) - \E[f(x)])^2]$
  \item[Find] $\var[f] = \E[f(x)^2] - \E[f(x)]^2$
\end{description}

For the expected value $\E$, random variables $a$ and $b$ and constant $c$ it holds that
\begin{align}
  \E[a+b] &= \E[a] + \E[b]       \label{eq:e-add} \\
  \E[c \cdot a] &= c \cdot \E[a] \label{eq:e-multi} \\
  \E[c] &= c                     \label{eq:e-const}
\end{align}

Followingly, we can prove the desired equation:
\begin{align*}
  \var[f] &= \E[(f(x) - \E[f(x)])^2] \\
          &= \E[f(x)^2 - 2f(x) \E[f(x)] + \E[f(x)]^2] \\
          &\overset{\eqref{eq:e-add}}{=} \E[f(x)^2] - \E[2 \cdot f(x) \cdot \E[f(x)]] + \E[f(x)]^2 \\
          &\overset{\eqref{eq:e-multi}}{=} \E[f(x)^2] - \E[2] \cdot \E[f(x)] \cdot \E[\E[f(x)]] + \E[f(x)]^2 \\
          &\overset{\eqref{eq:e-const}}{=} \E[f(x)^2] - 2 \cdot \E[f(x)] \cdot \E[f(x)] + \E[f(x)]^2 \\
          &= \E[f(x)^2] - \E[f(x)]^2
\end{align*}

\section{$\E[ab] = \E[a] \cdot \E[b]$ if $a$ and $b$ are independent}
%
\begin{description}
  \item[Given] two independent random variables $a$ and $b$
  \item[Find] $\E[a \cdot b] = \E[a] \cdot \E[b]$
\end{description}

Let $p(x)$ be the probability density function on $\mathbb{R}$
and $x$ be the $\mathbb{R}$-valued random variable (for $p$).
The expectation $\E[x]$ is defined by
\[ \E[x] = \int_{\mathbb R} p(x) \cdot x \, dx \]
We will prove the formula under this setting.
The case that the probability distribution is defined on ${\mathbb R}^d$
can be discussed analogously. The case that the probability distribution
is given on a finite set $\Omega$ can be discussed by replacing the
integral by the sum over the finite set $\Omega$.
Furthermore for two independent random variables $a$ and $b$, it holds that
\[ p(ab) = p(a) \cdot p(b) \]

\begin{align*}
  \E[x] &= \int p(x) \cdot x \, dx \\
  \E[a \cdot b]
    &= \int \left( p(ab) \cdot ab \right) \: d(ab) \\
    &= \int \int \left( p(a) \cdot p(b) \cdot a \cdot b \right)   \: da \, db \\
    &= \int \int \left( p(a) \cdot a \right)  \cdot \left(p(b) \cdot b\right)  \: da \, db \\
%    &= \int \left( \left( \int p(a) \cdot a \: da\right) \cdot p(b) \cdot b \right)   \: db \\
    &= \left(\int p(a) \cdot a \: da\right) \left(\int p(b) \cdot b  \: db\right) \\
    &= \E[a] \cdot \E[b]
\end{align*}

\section{Exercise 1.6}

\begin{quote}
  \enquote{For two random variables $x$ and $y$, the \emph{covariance} is defined by
  \begin{align*}
    \cov{[x,y]} &= \E_{x,y}[\{x - \E_{x}[x]\}\{y - \E_{y}[y]\}] \\
                &= \E_{x,y}[xy] - \E_{x}[x] \E_{y}[y]
  \end{align*}
  which expresses the extent to which $x$ and $y$ vary together.
  If $x$ and $y$ are independent, then their covariance vanishes.} \\
  ---page 20
\end{quote}

\begin{description}
  \item[Given] two independent random variables $x$ and $y$ with covariance defined by $\cov[x,y] = \E_{x,y}[\{x - \E_{x}[x]\}\{y - \E_{y}[y]\}]$
  \item[Find] $\cov[x,y] = 0$
\end{description}

If two random variables $a$ and $b$ are independent, then it holds that
\begin{align}
  \E[a \cdot b] &= \E[a] \cdot \E[b]       \label{eq:e-2-multi}
\end{align}

\begin{align*}
  \cov{[x,y]}
    &= \E_{x,y}[\{x - \E_{x}[x]\}\{y - \E_{y}[y]\}] \\
    &= \E_{x,y}[xy - y\E_x[x] - x\E_y[y] + \E_x[x] \cdot \E_y[y]] \\
    &\overset{\eqref{eq:e-add}}{=} \E_{x,y}[xy] + \E_{x,y}[-y \cdot \E_x[x]] + \E_{x,y}[-x \cdot \E_y[y]] + \E_{x,y}[\E_x[x] \cdot \E_y[y]] \\
    &\overset{\eqref{eq:e-multi}}{=} \E_{x,y}[xy] - \E_x[x] \cdot \E_{y}[y] - \E_{y}[y] \cdot \E_{x}[x] + \E_{x,y}[\E_x[x] \cdot \E_y[y]] \\
    &\overset{\eqref{eq:e-2-multi}}{=} \E_{x,y}[x] \cdot \E_{x,y}[y] - 2 \cdot \E_x[x] \cdot \E_{y}[y] + \E_{x,y}[\E_x[x] \cdot \E_y[y]] \\
    &\overset{\eqref{eq:e-const}}{=} \E_{x,y}[x] \cdot \E_{x,y}[y] - 2 \cdot \E_x[x] \cdot \E_{y}[y] + \E_x[x] \cdot \E_y[y] \\
    &= 0
\end{align*}

\section{Gaussian interpretation of curve fitting}

\begin{description}
  \item[Given] $p(t \given x,w,\beta) = \N(t \given y(x,w), \beta^{-1})$ and
    \[ \N(x \given \mu, \sigma^2) = (2 \pi \sigma^2)^{-\frac12} \exp\left(-2^{-1} \sigma^{-2} (x - \mu)^2\right) \]
  \item[Find]
    \[ \ln{p(t \given x,w,\beta)} = -\frac{\beta}{2} \cdot \sum_{n=1}^N \left(y(x_n,w) - t_n\right)^2 + \frac{N}{2} \ln\beta - \frac{N}{2} \ln(2\pi) \]
\end{description}

Remember the basic laws for logarithms:
\begin{align}
  \ln(a \cdot b) &= \ln{a} + \ln{b}              \label{eq:ln-multi} \\
  \ln\left(\frac{a}{b}\right) &= \ln{a} - \ln{b} \label{eq:ln-div} \\
  \ln(\exp(a)) &= a                              \label{eq:ln-inv} \\
  \ln(a^b) &= b \cdot \ln(a)                     \label{eq:ln-power}
\end{align}

\begin{align}
  \ln{p(t \given x,w,\beta)} &= (2 \pi \beta^{-1})^{-\frac12} \exp\left(-2^{-1} \beta (t - y(x,w))^2\right) \\
      &\overset{\eqref{eq:ln-multi}}{=} \ln(2 \pi \beta^{-1})^{-\frac12} + \ln\exp\left(-2^{-1} \beta (t - y(x,w))^2\right) \\
      &\overset{\eqref{eq:ln-inv}}{=} \ln\left(\frac{\beta}{2\pi}\right)^{\frac12} - \frac{\beta}{2}\left(t - y(x,w)\right)^2 \\
      &\overset{\eqref{eq:ln-inv}}{=} \frac12 \cdot \ln\left(\frac{\beta}{2\pi}\right) - \frac{\beta}{2}\left(t - y(x,w)\right)^2 \\
      &\overset{\eqref{eq:ln-inv}}{=} \frac12 \cdot \ln \beta - \frac12 \cdot \ln(2\pi) - \frac{\beta}{2}\left(t - y(x,w)\right)^2 \\
  \sum_{n=1}^N \ln{p(t_n \given x_n,w,\beta)} &= \sum_{n=1}^N \left(\frac12 \ln{\beta} - \frac12 \ln{2\pi} - \frac\beta2 (t_n - y(x_n, w))^2\right) \\
      &= \frac{N}{2} \ln\beta - \frac{N}{2} \ln{2\pi} - \frac{\beta}{2} \sum_{n=1}^N \left(y(w_n, w) - t_n\right)^2
\end{align}

\section{Exercise 1.11}

\begin{description}
  \item[Given] $\ln{p(x \given \mu, \sigma^2)} = -\frac1{2\sigma^2} \sum_{n=1}^N (x_n - \mu)^2 - \frac{N}{2} \ln{\sigma^2} - \frac{N}{2} \ln(2\pi)$
  \item[Find] $\mu_{\text{ML}} = \frac{1}{N} \cdot \sum_{n=1}^N x_n$ for maximized $\mu$ and \\
    $\sigma_{\text{ML}} = \frac{1}{N}\cdot \sum_{n=1}^N (x_n - \mu_{\text{ML}})^2$ for maximized $\sigma^2$
\end{description}

So we want to determine the 2 parameters of a Gaussian distribution, namely $\mu$ and $\sigma^2$, in the maximum likelihood case.
We begin with $\mu$:

\begin{enumerate}
  \item Derive $\ln{p(x \given \mu, \sigma^2)}$ for $\mu$
    \begin{align*}
      \frac\partial{\partial \mu} \ln{p(x \given \mu, \sigma^2)}
      &= \frac\partial{\partial \mu} \left(-\frac{1}{2\sigma^2} \cdot \sum_{n=1}^N (x_n - \mu)^2 - \frac{N}{2} \ln{\sigma^2} - \frac{N}{2} \ln(2\pi)\right) \\
      &= \frac\partial{\partial \mu} \left(-\frac{1}{2\sigma^2} \cdot \sum_{n=1}^N (x_n^2 - 2 x_n \mu + \mu^2) - \frac{N}{2} \ln{\sigma^2} - \frac{N}{2} \ln(2\pi)\right) \\
      &= -\frac{1}{2\sigma^2} \cdot \sum_{n=1}^N (-2x_n + 2\mu) \\
      &= -\frac1{\sigma^2} \cdot \sum_{n=1}^N (\mu - x_n)
    \end{align*}
  \item Set result zero
    \[ 0 = -\frac{1}{\sigma^2} \cdot \sum_{n=1}^N (\mu - x_n) = \sum_{n=1}^N (\mu - x_n) = N \cdot \mu - \sum_{n=1}^N x_n \]
    \[ \implies \mu_{\text{ML}} = \frac1N \cdot \sum_{n=1}^N x_n \qquad \text{commonly called \enquote{sample mean}} \]
\end{enumerate}

We continue with $\sigma^2$ and use the same approach:

\begin{enumerate}
  \item Derive $\ln{p(x \given \mu, \sigma^2)}$ for $\sigma^2$
    \begin{align*}
      \frac{\partial}{\partial \sigma^2} \ln{p(x \given \mu, \sigma^2)}
      &= \frac{\partial}{\partial \sigma^2} \left(-\frac{1}{2\sigma^2} \cdot \sum_{n=1}^N (x_n - \mu)^2 - \frac{N}2 \ln{\sigma^2} - \frac{N}2 \ln(2\pi)\right) \\
      &= \frac{1}{2\sigma^4} \cdot \sum_{n=1}^N (x_n - \mu)^2 - \frac{N}{2} \cdot \frac{1}{\sigma^2} \\
      &= \frac{1}{2\sigma^2} \left(\frac{1}{\sigma^2} \cdot \sum_{n=1}^N (x_n - \mu)^2 - N\right)
    \end{align*}
  \item Set result zero
    \begin{align*}
      0 &= \frac{1}{2\sigma^2} \left(\frac{1}{\sigma^2} \cdot \sum_{n=1}^N (x_n - \mu)^2 - N\right) \\
      N \cdot \sigma^2 &= \sum_{n=1}^N (x_n - \mu)^2 \\
      \sigma^2_{\text{ML}} &= \frac{1}{N} \cdot \sum_{n=1}^N (x_n - \mu)^2 \qquad \text{commonly called \enquote{sample variance}}
    \end{align*}
\end{enumerate}

\section{Precision parameter $\beta$ in the maximum likelihood case}

\begin{description}
  \item[Given] $\ln{p(t \given x,w,\beta)} = -\frac{\beta}{2} \cdot \sum_{n-1}^N \left(y(x_n,w) - t_n\right)^2 + \frac{N}2 \ln{\beta} - \frac{N}{2} \ln(2\pi)$
  \item[Find] $\frac{1}{\beta_{\text{ML}}} = \frac{1}{N} \cdot \sum_{n=1}^N (y(x_n, w_{\text{ML}}) - t_n)^2$ by maximizing $\beta$
\end{description}

\begin{enumerate}
  \item Derive $\ln{p(t \given x,w,\beta)}$ with $\beta$
    \begin{align*}
      \frac{\partial}{\partial \beta} \ln{p(t \given x,w,\beta)}
      &= \frac{\partial}{\partial \beta} \left(-\frac{\beta}{2} \sum_{n=1}^N (y(x_n,w) - t_n)^2 + \frac{N}{2} \ln\beta - \frac{N}{2} \ln(2\pi)\right) \\
      &= -\frac12 \cdot \sum_{n=1}^N \left(y(x_n,w) - t_n\right)^2 + \frac{N}{2} \cdot \frac1\beta
    \end{align*}
  \item Set result zero
    \begin{align*}
      0 &= -\frac{1}{2} \cdot \sum_{n=1}^N \left(y(x_n,w) - t_n\right)^2 + \frac{N}{2\beta} \\
      \frac{N}{\beta} &= \sum_{n=1}^N \left(y(x_n,w) - t_n\right)^2 \\
      \frac{1}{\beta_{\text{ML}}} &= \frac{1}{N} \cdot \sum_{n=1}^N (y(x_n,w) - t_n)^2
    \end{align*}
\end{enumerate}

\end{document}
