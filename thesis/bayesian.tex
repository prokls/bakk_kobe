\chapter{Bayesian theory}
\section{Probability theory and statistics}
\subsection[Ïƒ-algebra]{$\boldsymbol\sigma$-algebra}
%
The following mathematical object is necessary in order to define probability properly:

\begin{definition}
  Let $X$ be a set. A $\sigma$-algebra is a set $Y$ of subsets of $X$ satisfying:
  \begin{enumerate}
    \item $\emptyset \in Y$
    \item $Z \in Y \implies Z^C \in Y$ where $Z^C$ denotes the complement of $Z$, $X \setminus Z$.
    \item $\left(\bigcup_{i=1}^\infty Z_i\right) \in Y$ where $n \in \Nat$ and $Z_i \in Y$ where $i = 1, 2, \ldots$.
  \end{enumerate}
\end{definition}
When $X$ is a finite set, we may limit the third condition to a finite union.

\begin{example}
  Let $X \coloneqq \set{a, b, c, d}$ and $Z \coloneqq \set{\set{a}}$. We extend $Z$ to a $\sigma$-algebra $Y$:
  \[ Y = \set{\set{}, \set{a,b,c,d}, \set{a}, \set{b,c,d}} \]
\end{example}
The notion of the $\sigma$-algebra is essential to define the notion of the probability space and the random variable rigorously.
In this thesis, our discussion is rigorous when $X$ is a finite set.
When $X$ is an infinite set, we need a full general discussion of measure theory.
Then our discussion will sometimes be intuitive or informal.

\subsection{Basic definitions}
\label{sec:bp-def}
%
Probability theory is concerned with random experiments and random phenomena. Probability in its basic form is the fraction of events with a certain outcome to the total number of events.
%
\begin{definition}
  \label{def:prob}
  \index{Probability space}
  \index{Probability measure}
  \index{Event}
  \index{Random variable}
  A \emph{probability space} $(\Omega, \mathcal A, \Prob)$ denotes
    the set of possible outcomes, a set of events, and a map from an element of $\mathcal A$ to a real value in $[0,1]$.
    $\mathcal A$ is a $\sigma$-algebra. As elements of $\mathcal A$ are sets, we can apply set operations on them.
    If $\Omega$ is a finite space, \emph{probability measure} $\Prob$ satisfies the following conditions:
    \begin{align}
      &\Prob[A] \geq 0 \text{ for } A \in \mathcal A \label{def:prob-1} \\
      &\Prob[A + B] = \Prob[A] + \Prob[B] \text{ for } A, B \in \mathcal A \text{ and } A \cap B = \emptyset \label{def:prob-2} \\
      &\Prob[\Omega] = 1 \label{def:prob-3}
    \end{align}
    Property~\ref{def:prob-2} implies linearity of the probability measure (for mutually exclusive events $A$):
    \begin{align}
      \Prob\left[\bigcup A\right] &= \sum \Prob\left[A\right] \label{eq:prob-linear}
    \end{align}
  An \emph{event} is any subset $a$ of $\Omega$, hence $a \in \mathcal A$.
  A \emph{$\mathbb Z$-valued random variable $R$} is a map from $\Omega$ to $\mathbb Z$
  such that $R^{-1} \in \mathcal A$ for any $z \in \mathbb Z$.
  A \emph{$\mathbb R$-valued random variable $R$} is a map from $\Omega$ to $\mathbb R$
  such that $R^{-1}((r, s]) \in \mathcal A$ for any real numbers $r < s$.
\end{definition}
\begin{example}
  \label{ex:cointoss}
  A coin toss has two possible outcomes, heads (\textit h) or tails (\textit t).
  We consider two coin tosses.
  Then $\Omega = \set{(h, h), (h, t), (t, h), (t, t)}$ and $\mathcal A$ is the powerset of $\Omega$ (i.e. set of all subsets).
  Let $\Prob[A] = \#A / 4$, the size of set $A$ divided by $4$.
  Let $R$ be our random variable in $\mathbb Z$ defined as result of the first coin toss (1 represents head, 0 represents tails).
  Then $\Prob[R=1] = \Prob[\setdef{\omega \in \Omega}{R(\omega) = 1}] = \Prob[\set{(h, h), (h, t)}] = \frac24$.
\end{example}

In the following, we will declare random variables, but won't specify the group explicitly.
Either it is obvious from context (because of the numbers, we use) or
our statements work for both groups.

\subsection{Average Value and Expected Value}
\label{sec:bp-ae}
%
\begin{definition}
  \index{Average value}
  \index{Population mean}
  \index{Sample mean}
  \index{Expected value}
  Let $\Omega$ be a finite set and $R$ be an $E$-valued random variable where $E$ is $\mathbb Z$ or $\mathbb R$.
  The \emph{average value} $\avg{R}$ of a random variable $R$ is defined as,
  \begin{align}
    \avg{R} &\coloneqq \frac{1}{\card{\mathcal A}} \sum_{a \in \mathcal A} R(a)
  \end{align}
  If all outcomes of the sample space $\Omega$ are considered, we call $\avg{R}$ the \emph{population mean} (denoted $\mu$),
  otherwise \emph{sample mean}. \\
  We define the \emph{expected value} of a random variable $R$ as follows and denote it by $\E$.
  \begin{align}
    \E[R] &\coloneqq \mu = \sum_{a \in \mathcal A} \Prob[a] \cdot R(a) \\
          &= \sum_{z \in E} \Prob[R = z] \cdot z  \notag
  \end{align}
  where $\Prob[R = z]$ is the probability of the random variable $R$ taking the value $z$.
  In other words, $\Prob[R = z] \coloneqq \Prob[A_z]$ with $A_z \coloneqq \setdef{e \in \Omega}{R(e) = z}$.
\end{definition}

Let $R$ and $S$ be two random variables and $c \in \mathbb R$.
The following properties are satisfied:
\begin{align}
  \E[c]     &\coloneqq c \\
  \E[R + c] &\coloneqq \sum_{a \in \mathcal A} \left(\Prob[R=a] \cdot (a + c)\right) \notag\\
            &= \sum_{a \in \mathcal A} \left(\Prob[R=a] \cdot a\right) + \sum_{a \in \mathcal A} \left(\Prob[R=a] \cdot c\right) \notag\\
            &= \E[R] + c \cdot \sum_{a \in \mathcal A} \Prob[R=a] \notag\\
            &= \E[R] + c \cdot 1 = \E[R] + c \\
  \E[R + S] &\coloneqq \sum_{a \in (\mathcal A_R \cup \mathcal A_S)}
              \begin{cases}
                \Prob[R=a] \cdot a & \text{if } a \in A_R \\
                \Prob[S=a] \cdot a & \text{if } a \in A_S
              \end{cases}\notag\\
            &= \sum_{a \in \mathcal A_R} \Prob[R=a] \cdot a + \sum_{a \in \mathcal A_S} \Prob[S=a] \cdot a \notag\\
            &= \E[R] + \E[S] \\
  \E[c \cdot R] &\coloneqq \sum_{a \in \mathcal A} \Prob[R=a] \cdot (c \cdot a) \notag\\
            &= c \cdot \sum_{a \in \mathcal A} \Prob[R=a] \cdot a = c \cdot \E[R]
\end{align}

\subsection{Equivalence of the continuous probability model}
\label{sec:bp-continuous}
%
\begin{definition}
  \index{Probability Density Function}
  Let $R$ be an $\mathbb R$-valued random variable and
  $f$ be a continuous function defined in $(-\infty, \infty) \subseteq \mathbb R$.
  Let $f$ satisfy the following properties:
  \[
    \Prob[R \leq y] \coloneqq \int\limits_{-\infty}^y f(x) \, dx \hspace{28pt}
    \Prob[z \leq R \leq y] \coloneqq \int\limits_z^y f(x) \, dx \hspace{28pt}
    \Prob[z \leq R] \coloneqq \int\limits_z^\infty f(x) \, dx
  \]
  This establishes a relation between function $f$ and random variable $R$.
  Because $R$ satisfies properties~\ref{def:prob-1} and \ref{def:prob-3} of probability measures,
  $f$ also satisfies:
  \begin{align}
    f(x) &\geq 0 \qquad \forall x \in (-\infty, \infty) \label{prop:pdf1} \\
       1 &= \int_{-\infty}^{\infty} f(x) \, dx \label{prop:pdf2}
  \end{align}
  $f$ is called a \emph{Probability Density Function} (PDF).
\end{definition}
\begin{example}
  \[ f(x \; | \; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2} } \; e^{ -\frac{(x-\mu)^2}{2\sigma^2} } \]
  The normal distribution function depending on parameters $\mu$ and $\sigma^2$ is one example for a probability density function.
  The function is introduced in Section~\ref{sec:bp-norm-dist} in detail.
\end{example}

We have seen that the continuous model follows equivalent properties.
The same is true for the expected value.

\begin{definition}
  \index{Expected value}
  The \emph{expected value} in the continuous model is defined as,
  \begin{align}
    \E[R] &\coloneqq \int_{-\infty}^\infty \left(\Prob[R=x] \cdot x\right) \, dx
          = \int_{\mathbb R} \left(\Prob[R=x] \cdot x\right) \, dx
  \end{align}
  where $z \in E$ where $E$ is $\mathbb Z$ or $\mathbb R$ (group of the random variable).
\end{definition}
%
\begin{proof}
  Let $R$ and $S$ be two random variables and $c \in \mathbb R$.
  The expected value satisfies:
  %
  \begin{align}
    \E[c]     &\coloneqq c \label{eq:Ec} \\
    \E[R + c] &\coloneqq \int_{\mathbb R} \Prob[R=x] \cdot (x + c) \, dx \notag\\
              &= \int_{\mathbb R} \left(\Prob[R=x] \cdot x + \Prob[R=x] \cdot c\right) \, dx \notag\\
              &= \int_{\mathbb R} \Prob[R=x] \cdot x \, dx + c \cdot \int_{\mathbb R} \Prob[R=x] \, dx \notag\\
              &= \E[R] + c \cdot 1 = \E[R] + c \\
    \E[R + S] &\coloneqq \int_{\mathbb R} \left(\Prob[R=x] \cdot x + \Prob[S=x] \cdot x\right) \, dx \notag\\
              &= \int_{\mathbb R} \Prob[R=x] \cdot x \, dx + \int_{\mathbb R} \Prob[S=x] \cdot x \, dx \notag\\
              &= \E[R] + \E[S] \label{eq:Evv} \\
    \E[c \cdot X] &\coloneqq \int_{\mathbb R} \Prob[R=x] \cdot (x \cdot c) \, dx \notag\\
              &= c \cdot \int_{\mathbb R} \Prob[R=x] \cdot x \, dx
              = c \cdot \E[X] \label{eq:Ecv}
  \end{align}
\end{proof}
Because $\Prob$ and $\E$ provide the same properties in the discrete and continuous case,
we often do not distinguish between these cases. The statements hold true for $\mathbb Z$-valued
as well as $\mathbb R$-valued random variables.

\subsection{Variance and standard deviation}
\label{sec:bp-var-sd}
%
\begin{definition}
  \index{Variance}
  \emph{Variance} quantifies how strong values are spread out from $\E[R]$: %the expected value $\E[R]$:
  \[ \sigma^2 \coloneqq \E\left[(R - \E[R])^2\right] \]
\end{definition}
Considering the entire population, the variance can also quantify over the population mean $\mu$:
\[ \sigma^2 = \Var[R] \coloneqq \E[(R - \mu)^2] \]
In the discrete case, this is equivalent to,
\[ \Var[R] = \E\left[\sum_{a \in \mathcal A} \left(\Prob[R=a] \cdot (a - \mu)\right)^2\right] \]
and in the continuous case, we have:
\[ \Var[R] = \E\left[\int_{\mathbb R} \left(\Prob[R=x] \cdot (x + \mu)\right)^2 \, dx\right] \]
\begin{definition}
  \index{Standard deviation}
  The \emph{standard deviation} is defined as its second root:
  \[ \sd = \sqrt{\Var[R]} \]
\end{definition}

\subsection{Covariance}
\label{sec:covar}
%
\index{Covariance}
\emph{Covariance} measures the joint variability of two given random variables.
It is defined as:
\begin{align}
  \Cov[X,Y]
    &= \E[(X - \E[X])(Y - \E[Y])] \label{eq:cov}\\
    &= \E[XY - Y\cdot\E[X] - X\cdot\E[Y] + \E[X]\E[Y]] \notag\\
    &= \E[XY] - \E[X]\cdot\E[Y] \label{eq:cov-exy}
\end{align}
%
If $X$ and $Y$ are independent (compare with Section~\ref{sec:bp-indep}), then the covariance is zero.
\begin{align}
  \Cov[X,Y] &= \E[XY] - \E[X] \cdot \E[Y] = \E[X] \cdot \E[Y] - \E[X] \cdot \E[Y] = 0 \label{eq:CovXY0}
\end{align}
We will exploit the following properties:
\begin{align}
  \Cov[X,Y] &= \E[(X - \E[X])(Y - \E[Y])] = \E[(Y - \E[Y])(X - \E[X])] = \Cov[Y,X] \\
  \Cov[X,X] &= \E[(X - \E[X])^2] = \Var[X]
\end{align}
%
Let $X$ be a set of $n$ independent variables $X_{1 \leq i \leq n}$.
Then it holds that:
\begin{align}
  \Var\left[\sum_{i=1}^n X_i\right]
    &= \E\left[\left(\sum_{i=1}^n X_i - \E\left[\sum_{i=1}^n X_i\right]\right)^2\right] \notag \\
    &= \E\left[\left(\sum_{i=1}^n \left(X_i - \E\left[X_i\right]\right)\right)^2\right] \notag \\
    &= \E\left[\sum_{i=1}^n \left(X_i - \E[X_i]\right) \cdot \sum_{j=1}^n \left(X_i - \E[X_i]\right)\right] \notag \\
    &= \E\left[\sum_{j=1}^n \left(\sum_{i=1}^n (X_i - \E[X_i])\right) (X_j - \E[X_j]) \right] \notag \\
    &= \E\left[\sum_{i,j \in [1,n]} \left(X_i - \E[X_i]\right) \left(X_j - \E[X_j]\right)\right] \notag \\
    &= \sum_{i,j \in [1,n]} \E[(X_i - \E[X_i])(X_j - \E[X_j])] \notag \\
    &= \sum_{i,j \in [1,n]} \Cov[X_i, X_j] \notag \\
    &= \sum_{\substack{i \neq j \\ i,j \in [1,n]}} \Cov[X_i, X_i] + \sum_{\substack{i \neq j \\ i,j \in [1,n]}} \Cov[X_i, X_j] \notag \\
    &= \sum_{i=1}^n \Var[X_i] + \sum_{\substack{i \neq j \\ i,j \in [1,n]}} \Cov[X_i, X_j] \notag \\
    &= \sum_{i=1}^n \Var[X_i] + 2 \cdot \sum_{1 \leq i < j \leq n} \Cov[X_i, X_j] \label{eq:sumVarCov}
\end{align}

\subsection{Law of Large Numbers}
\label{sec:bp-lln}
%
The Law of Large Numbers stresses the practical importance of the expected value.
\begin{theorem}[Law of Large Numbers]\label{thm:lln}
  First, we define the notion of the average value over a sample $A_i$ of size $n$:
  \[ \avg{R}_n \coloneqq \frac1n \sum_{i=0}^n A_i \]
  Then, the Law of Large Numbers states that,
  \[ \lim_{n\to\infty} \avg{R}_n = \E[R] \]
\end{theorem}

In order to prove this theorem, we use Chebyshev's Inequality, the Weak Law of Large numbers,
Borel-Cantelli Lemma and the Strong Law of Large Numbers. The latter is considered equivalent
to the Law of Large numbers~\footnote{
  Depending on your requirements of certainty, the Weak Law of Large numbers might be already
  considered equivalent to the Law of Large Numbers (Theorem~\ref{thm:lln}), but we look
  for the Strong Law of Large Numbers in this thesis.
}. Our proof structure is based on Craig A. Tracy's~\cite{cnfgen}\footnote{
  Please recognize that there is a small typographical error on page~3.
  \enquote{$S_n(\omega) = 1$ for every $n$} should be \enquote{$X_n(\omega) = 1$ for every $n$}.
}.

\subsubsection{Chebyshev's Inequality}
\label{sec:bp-chebyshev}
%
Consider the continuous case.
Let $R$ be a random variable, $f$ be a PDF over $R$, $|\cdot|$ be a norm,
$a \in \mathbb R_{\geq 0}$, $p \in \mathbb N$ and let $\E[R^p]$ be defined as follows:
\begin{align*}
  \E[R^p]
          = \int\limits_{\mathbb R} x^p \cdot f(x) \, dx 
          \geq \int\limits_{x \geq a} x^p \cdot f(x) \, dx 
          \geq a^p \int\limits_{x \geq a} f(x) \, dx 
          = a^p \cdot \Prob[R \geq a]
\end{align*}
The discrete case follows immediately.
This concludes the correctness of the following theorem:
\begin{theorem}[Chebyshev's Inequality Theorem]\label{thm:chebyshev}
  Let $R$ be a random variable, $a \in \mathbb R_{\geq 0}$ and $p \in \mathbb N$ is arbitrary.
  Assume $\E[R^p] < \infty$.
  Then it holds that,
  \[ \Prob[R \geq a] \leq \frac{1}{a^p} \E[R^p] \]
\end{theorem}

\subsubsection{Weak Law of Large Numbers}
\label{sec:bp-weak-law}
%
The next theorem is called Weak Law of Large Numbers.
\begin{theorem}[Weak Law of Large Numbers, Bernoulli's Theorem]\label{thm:weak-lln}
  Let $R_i$ be a sequence of independent and identically distributed random variables
  (see section~\ref{sec:bp-iid} for a definition of i.i.d.)
  with common mean $\mean$ and variance $\var$. Let
  \[
      S_n \coloneqq \sum_{i=1}^n R_i \hspace{50pt}
      T_n \coloneqq \frac{S_n}{n} - \mean
  \]
  Then for any $\varepsilon > 0$,
  \[ \lim_{n\to\infty} \Prob[T_n \geq \varepsilon] = 0 \]
\end{theorem}

\begin{proof}
  First, we determine the expected values.
  \begin{align}
      \E[S_n] &= \E\left[\sum_{i=1}^n R_i\right] = \sum_{i=1}^n \E[R_i] = \sum_{i=1}^n \mu = n \cdot \mu & \eqref{eq:Evv} \label{eq:ES_n} \\
%      \E[T_n] &= \E\left[\frac{1}{n}\left(R_1 + R_2 + \ldots + R_n\right) - \mu\right] \notag\\
%              &= \frac1n \left(\E[R_1] + \E[R_2] + \ldots + \E[R_n]\right) - \E[\mu] & \eqref{eq:Ecv} \notag\\
%              &= \frac{n \cdot \mu}{n} - \mu = 0 \label{eq:tn0}
      \E[T_n] &= \E\left[\frac{S_n}{n} - \mu\right] = \frac{\E\left[S_n\right]}{\E\left[n\right]} - \E\left[\mu\right] & \eqref{eq:Evv} \notag\\
              &= \frac{n \cdot \mu}{n} - \mu = 0  & \eqref{eq:ES_n}, \eqref{eq:Ecv} \label{eq:tn0}
  \end{align}
  We also need a result regarding the variance.
  In the continuous and discrete case, it holds that $a^2 \cdot \Var[X] = \Var[a \cdot X]$:
  \begin{align}
    a^2 \cdot \Var[X] &= a^2 \cdot \int (x - \mu)^2 \cdot f(x) \, dx = \int (x \cdot a - \mu \cdot a)^2 \cdot f(x) \, dx \notag \\
    a^2 \cdot \Var[X] &= a^2 \cdot \sum_{i=1}^n p_i \cdot (x_i - \mu)^2 = \sum_{i=1}^n p_i \cdot a^2 \cdot (x_i - \mu)^2 = \sum_{i=1}^n p_i \cdot (x_i \cdot a - \mu \cdot a)^2 \label{eq:a2V}
  \end{align}
  The relation $\E[X^2] = \Var[X] + \E[X]^2$ holds as well,
  \begin{align}
    \Var[X] &= \E[(X - \mu)^2] = \E[X^2] - \E[2X\mu] + \E[\mu^2] \notag & \eqref{eq:Evv} \\
            &= \E[X^2] - 2 \cdot \E\left[X \cdot \E[X]\right] + \E\left[\E[X]^2\right] & \eqref{eq:Ecv} \notag\\
            &= \E[X^2] - 2 \cdot \E[X]^2 + \E[X]^2 & \eqref{eq:Ecv} \notag\\
            &= \E[X^2] - \E[X]^2 \label{eq:VXisEX2-EX2}
  \end{align}
  We use this result to prove $\Var[T_n] = \frac{\sigma^2}{n}$.
  \begin{align}
    \Var[T_n] &= \E[T_n^2] - \E[T_n]^2 = \E[T_n^2] - 0^2 = \E[T_n^2] & \eqref{eq:VXisEX2-EX2} \eqref{eq:tn0} \notag\\
              &= \E\left[\left(\frac{S_n}{n} - \mu\right)^2\right] = \E\left[\left(\frac{S_n}{n}\right)^2 - 2\frac{S_n}{n}\mu + \mu^2\right] \notag \\
              &= \E\left[\left(\frac{S_n}{n}\right)^2\right] - 2\cdot\E\left[\frac{\mu}{n} S_n\right] + \E\left[\mu^2\right] & \eqref{eq:Evv}\eqref{eq:Ecv} \notag\\
              &= \Var\left[\frac{S_n}{n}\right] + \E\left[\frac{S_n}{n}\right]^2 - 2\mu \cdot \E\left[\frac{S_n}{n}\right] + \mu^2 & \eqref{eq:VXisEX2-EX2}\eqref{eq:Ec} \notag \\
              &= \frac1{n^2} \cdot \Var\left[S_n\right] + \left(\frac1{n} \cdot \E\left[S_n\right]\right)^2 - \frac{2\mu}{n} \cdot (n \cdot \mu) + \mu^2 & \eqref{eq:Ecv}\eqref{eq:a2V} \notag \\
              &= \frac1{n^2} \cdot \left(\Var\left[S_n\right] + \E[S_n]^2\right) - 2\mu^2 + \mu^2 \notag \\
              &= \frac{1}{n^2} \left(\Var\left[\sum_{i=1}^n R_i\right] + \E[S_n]^2\right) - \mu^2 \notag\\
              &= \frac{1}{n^2} \left(\sum_{i=1}^n \Var[R_i] + 2\sum_{\substack{i<j \\ i,j=1}}^n \Cov[R_i,R_j] + \E[S_n]^2\right) - \mu^2 & \eqref{eq:sumVarCov} \notag\\
              &= \frac{1}{n^2} \left(\sum_{i=1}^n \sigma^2 + 0 + (n \cdot \mu)^2\right) - \mu^2 & \eqref{eq:ES_n} & \eqref{eq:CovXY0} \notag\\
              &= \frac{1}{n^2} \left(n \cdot \sigma^2 + n^2 \cdot \mu^2\right) - \mu^2 = \frac{\sigma^2}{n} + \mu^2 - \mu^2 = \frac{\sigma^2}{n}
  \end{align}
\end{proof}
%
%% REMARK: Yes, this is a quote from Wikipedia. Yes, it is true. But it is too short, because we didn't show that the covariance is zero and whether they are uncorrelated or not. So I prefer the approach above
%\index{Bienaym\'e formula}
% This proves the so-called Bienaym\'e formula.
% A shorter approach uses the definition of the mean:
% \[ \Var[\avg{T_n}] = \Var\left(\frac1n \sum_{i=1}^n X_i\right) = \frac1{n^2} \sum_{i=1}^n \Var[X_i] = \frac{\sigma^2}{n} \]

Furthermore, the following equation holds:
\begin{align}
    \Var[T_n] &= \E\left[(T_n - \E[T_n])^2\right] = \E\left[(T_n - 0)^2\right] = \E\left[T_n^2\right] \label{eq:EVT_n}
\end{align}
%
Now we can apply Chebyshev's Inequality Theorem ($R = T_n$, $a = \varepsilon \in \mathbb R, p = 2$):
\begin{align}
  \Prob[\abs{T_n} \geq \varepsilon]
    &\leq \frac{1}{\varepsilon^2} \E\left[\abs{T_n}^2\right]
    = \frac{1}{\varepsilon^2} \Var\left[\abs{T_n}\right]
    = \frac{1}{\varepsilon^2} \frac{\sigma^2}{n}
    & \eqref{eq:EVT_n} \label{eq:cheby-applied}
\end{align}
For any $\varepsilon > 0$ with $n \to \infty$, it holds that
\[ \Prob[\abs{T_n} \geq \varepsilon] \to 0 \]
\[
    \Leftrightarrow \forall \varepsilon > 0:
    \lim_{n\to\infty} \Prob[\abs{T_n} \geq \varepsilon] = 0
\]

This concludes the proof of the Weak Law of Large Numbers.
In order to finish our proof of the Strong Law of Large numbers,
we will use the Borel-Cantelli Lemma, an important result of measure theory.

\subsubsection{Borel-Cantelli Lemma}
\label{sec:bp-borel-cantelli}
%
\begin{lemma}[Borel-Cantelli Lemma]\label{lemma:bcl}
  Let $R_i$ with $1 \leq i < \infty$ be a sequence of events.
  Assume the sum of these probabilities is finite, then it holds that:
  \begin{align}
    \sum_{i=1}^\infty \Prob[R_i] < \infty
    &\implies
    \Prob\left(\limsup\limits_{i\to\infty} R_i\right) = 0
  \end{align}
  So the probability, that the occuring event is an event which occurs infinitely often, is 0.
\end{lemma}

\begin{proof}
  Please consider, that the limsup is defined as,
  \begin{align}
    \limsup\limits_{i\to\infty} R_i &\coloneqq \bigcap_{j=1}^\infty \bigcup_{i \geq j}^\infty R_i \label{eq:limsup-char}
  \end{align}

  The condition requires, that $\sum_{i=1}^\infty \Prob[R_i] < \infty$.
  This statement is equivalent to
  \begin{align}
    \inf_{j \geq 1} \sum_{i=j}^\infty \Prob[R_i] &= 0  \label{eq:inf0}
  \end{align}

  We can make our final conclusion to prove the Borel-Cantelli Lemma:
  \begin{align}
    \Prob\left[\limsup_{i\to\infty} R_i\right]
      &= \Prob\left[\bigcap_{j=1}^\infty \bigcup_{i=j}^\infty R_i\right] & \eqref{eq:limsup-char} \notag\\
      &\leq \inf_{j \geq 1} \Prob\left[\bigcup_{i=j}^\infty R_i\right] & \eqref{eq:prob-linear}\notag\\
      &\leq \inf_{j \geq 1} \sum_{i=j}^\infty \Prob\left[R_i\right] \notag\\
      &= 0 & \eqref{eq:inf0}
  \end{align}
  The result of an intersection of elements creates a set, which
  is an actual subset in any of these sets. However, the infimum
  is not necessarily an element of the set. Hence, an inequality
  is introduced in the second line.
\end{proof}

\subsubsection{Strong Law of Large Numbers}
\label{sec:bp-strong-law}
%
\begin{theorem}[Strong Law of Large Numbers]\label{thm:slln}
  Assume the definitions of Theorem~\ref{thm:weak-lln}.
  Therefore, $R_1, R_2, \ldots$ is an infinite sequence of independent random variables
  with a common distribution ($\mu = \E[R_j]$, $\sigma^2 = \Var[R_j]$). $S_n$ and $T_n$
  are defined. Now consider event $\mathcal E$:
  \[
    \mathcal E = \set{
      \omega \in \Omega: \lim_{n\to\infty} \frac{S_n(\omega)}{n} = \mu
    }
  \]
  Then it holds that
  \[ \Prob[\mathcal E] = 1 \]
\end{theorem}

\begin{proof}
  The following proof assumes $\sigma^2 = \E[R_j^2] < \infty$ and $\E[R_j^4] < \infty$.
  This restriction makes our proof easier, but the less restricted case $\E[R_j] < \infty$ suffices as assumption
  (but this is not proven in this thesis).

  Without loss of generality we assume $\mu = 0$. \\
  If $\mu = 0$ is not satisfied, we consider $P_j \coloneqq R_j - \mu$ instead.

  Now, we want to give a brief outline of the proof. If it holds that,
  \[ \lim_{n\to\infty} \frac{S_n(\omega)}{n} \neq 0 \]
  then $\exists \varepsilon \in \mathbb R$ with $\varepsilon > 0$ such that for infinitely many $n$
  \[ \frac{S_n(\omega)}{n} > \varepsilon \]
  So to prove the theorem, we will prove that for every $\varepsilon > 0$,
  \[ \Prob[S_n > n \cdot \varepsilon \text{ infinitely often}] = 0 \]
  In the following, this reveals that
  \[ \Prob[\mathcal E] = \Prob\left[\frac{S_n}{n} = 0\right] = 1 \]
  proving Theorem~\ref{thm:slln}. Hence condition $\frac{S_n}{n} = 0$ holds with probability $1$.

  First of all, we define
  \[ A_n = \set{\omega \in \Omega: S_n \geq n \cdot \varepsilon} \]
  and look at $\Prob[A_n]$ using the Chebyshev inequality (Theorem~\ref{thm:chebyshev}) with $p=4$ and $a=n\cdot\varepsilon$:
  \[ \Prob[S_n \geq (n\cdot\varepsilon)] \leq \frac{1}{(n\cdot\varepsilon)^4} \E[S_n^4] \]
  We must determine $\E[S_n^4]$ which equals to
  \begin{align*}
    \E\left[\sum_{1 \leq i,j,k,l \leq n} R_i R_j R_k R_l\right]
      &= \E\left[\sum_{1\leq i}^n \sum_{1\leq j}^n \sum_{1\leq k}^n \sum_{1\leq l}^n R_i R_j R_k R_l\right] \notag\\
      &= \E\left[(R_1^4 + \ldots + R_1 R_n^3) + (R_2 \ldots) + (R_3 \ldots) + (R_n \ldots + R_n^4) \right]
  \end{align*}
  Because $\E[R_i] = 0$, we can remove all terms containing $R_j$ of degree 1 for any $j$.
  These are terms of the structure (assuming $i, j, k$ and $l$ distinct),
  \[
    \E[R_i^3 R_j], \,
    \E[R_i^2 R_j R_k], \,
    \E[R_i R_j R_k R_l]
  \]
  Remember that multiplication of expected values (compare with Equation~\ref{eq:CovXY0}) applies here.
  The non-zero terms are $\E[R_i^4]$ and $\E[R_i^2 R_j^2] = \left(\E[R_i^2]\right)^2$.
  Now, we need to quantify the occurences of these non-zero terms. $\E[R_i^4]$ occurs $n$ times.
  Terms $\E[R_i^2 R_j^2]$ occur $3n \cdot (n-1)$ times, as there are $\frac{(n-1) \cdot n}{2}$ ways to choose 2 indices and 6 ways to find $R_i^2 R_j^2$.
  In conclusion, we determined,
  \[
    \E\left[S_n^4\right]
      = n \cdot \E\left[R_1^4\right] + 3n \cdot (n - 1) \cdot \sigma^4
      = n \cdot \left( \E\left[R_1^4\right] + 3n \cdot \sigma^4 - 3 \sigma^4 \right)
  \]
  In this expression, $n$ is our only constant occuring with polynomial degree $2$. So for any $n$ sufficiently large, there exists $C \in \mathbb R$ such that
  \begin{align}
    3 \sigma^4 n^2 + \left(\E\left[R_1^4\right] - 3 \sigma^4\right) \cdot n &\leq C \cdot n^2 \label{eq:3sigman2}
  \end{align}
  \[ \Rightarrow \E\left(S_n^4\right) \leq C n^2 \]

  We return back to Chebyshev's inequality
  \[
      \Prob[S_n \geq (n\cdot\varepsilon)]
      \leq \frac{1}{(n\cdot\varepsilon)^4} \E[S_n^4]
      \leq \frac{C \cdot n^2}{\varepsilon^4 \cdot n^2 \cdot n^2}
  \]
  It follows that, there exists some $n_0$ such that Equation~\eqref{eq:3sigman2} is satisfied.
  With this approach, we skip a finite number of elements of the sum. This does not affect its convergence or divergence.
  \[
      \sum_{n \geq n_0} \Prob\left[S_n \geq n \cdot \varepsilon\right]
      \leq \sum_{n \geq n_0} \frac{C}{\varepsilon^4 n^2} < \infty
  \]
  Therefore, the conditions to apply the Borel-Cantelli Lemma (Lemma~\ref{lemma:bcl}) are satisfied.
  For every $\varepsilon > 0$ it holds that,
  \[ \Prob\left[S_n \geq n \varepsilon \text{ infinitely often}\right] = 0 \]
\end{proof}

\subsection{Marginalization}
\label{sec:bp-marginalization}
%
\begin{definition}
  \index{Marginalization}
  Let $R$ and $S$ be two random variables. Then we define,
  \[
    \Prob[R=r, S=s] \coloneqq \Prob[\setdef{\omega \in \Omega}{R(\omega) = r \land S(\omega) = s}]
  \]
  This definition enables us to define \emph{marginalization}:
  \[ \Prob[R=r] = \sum_{s \in S} \Prob[R=r, S=s] \]
\end{definition}
\begin{example}
  Please consider our coin tossing example again.
  Let $R$ be our random variable in $\mathbb Z$ defined as result of the first coin toss
  (1 represents head, 0 represents tails).
  $\mathbb Z$-valued random variable $S$ is defined as result of the second coin toss.
  Then
  \[
    \Prob[R=1, S=0] = \Prob[\setdef{\omega \in \Omega}{R(\omega) = 1 \land S(\omega) = 0}]
                    = \Prob[\set{(h, t)}] = \frac14
  \]
  Marginalization applies, if we query $\Prob[R=1]$:
  \[
    \Prob[R=1] = \Prob[R=1, S=0] + \Prob[R=1, S=1] = \Prob[\set{(h, t)}] + \Prob[\set{(h, h)}] = \frac14 + \frac14 = \frac24
  \]
\end{example}

\subsection{Joint distribution}
\label{sec:bp-joint-distribution}
%
\begin{definition}
  Let $R$ and $S$ be two random variables.
  Joint distribution is given by the following definition:
  \[ \Prob[R=r, S=s] = \Prob[R=r] \cdot \Prob[S=s] \]
  It is important to recognize, that we assume conditional independence of events
  as defined in Section~\ref{sec:bp-indep}.
\end{definition}
\begin{example}
  In our coin tossing example, we have that:
  \[ \Prob[R=1, S=0] = \Prob[\set{(h, t), (h, h)}] \cdot \Prob[\set{(t, t), (h, t)}] = \frac24 \cdot \frac24 = \frac12 \]
  In the Marginalization example, we used the same query and used a different approach to get the same result.
\end{example}

\subsection{independence}
\label{sec:bp-indep}
%
\index{Independence}
\index{Mutual independence}
\index{Conditional independence}
\begin{definition}
  We assume $\Omega$ is a finite set and consider
  the probability space $(\Omega, \mathcal A, \Prob)$.
  Two $\mathbb Z$-valued random variables $R$ and $S$ are called \emph{(mutually) independent} if
  \[ \Prob[R=r, S=s] = \Prob[R=r] \cdot \Prob[S=s] \]
  for any $r, s \in {\mathbb Z}$ (see, e.g., \cite[p.~27]{ito}, \cite[p.~143]{grindstead}).
\end{definition}

Let $A$ be an element of the $\sigma$-algebra ${\mathcal A}$.
We denote by $1_A$ the indicator function of $A$ defined as
\[
  1_A(a) = \begin{cases}
    1 & \text{ if } a \in A \\
    0 & \text{ if } a \not\in A
  \end{cases}
\]
The indicator function $1_A$ defines a ${\mathbb Z}$-valued random variable.
Let $A$ and $B$ be elements of the $\sigma$-algebra ${\mathcal A}$.
The sets (events) $A$ and $B$ are called independent
when the random variables $1_A$ and $1_B$ are independent.

\begin{example}
  Consider our previous coin tossing example (compare with Example~\ref{ex:cointoss}).
  Let us denote coin tosses $ij$, where $ij$ are the results of the first and second coin toss respectively.
  Define the random variable $R$ as
  \[ R(00)=0, R(01)=0, R(10)=1, R(11)=1 \]
  and define the random variable $S$ (intuitively the result of the second toss) as
  \[ S(00)=0, S(01)=1, S(10)=0, S(11)=1 \]
  Then we have
  \[
    \Prob(R=0, S=0) = 
    \Prob(R=0, S=1) = 
    \Prob(R=1, S=0) = 
    \Prob(R=1, S=1) = 1/4
  \]
  Since $\Prob(R=i) = \frac12$ and $\Prob(S=j) = \frac12$,
  we can see $\Prob(R=i, S=j) = \Prob(R=i) \cdot \Prob(S=j)$.
  Therefore $R$ and $S$ are independent random variables.
  The elements of $\sigma$ algebra
  $A = \set{01, 11} = R^{-1}(1)$ and $B = \set{10, 11} = S^{-1}(1)$
  are independent.
  Intuitively speaking, the event getting $1$ at the first toss
  and the event getting $1$ at the second toss
  are independent.
\end{example}

Let $R$, $S$, $T$ be $\mathcal Z$-valued random variables.
The random variables $R$ and $S$ are conditionally independent under the given random variable $T$
when
\[
  \ProbCond{R=r, S=s}{T=t} =
  \ProbCond{R=r}{T=t} \cdot \ProbCond{S=s}{T=t}
\]
for any $r, s, t \in \mathcal Z$ (see, e.g., \cite[3.1]{dawid}).
Let
\[
  A_r = R^{-1}(r) \subset \Omega \qquad
  B_s = S^{-1}(s) \subset \Omega \qquad
  C_t = T^{-1}(t) \subset \Omega
\]
Then $\Prob[A_r]$, $\Prob[B_s]$, $\Prob[C_t]$ can be regarded as probability distribution functions associated to $R, S, T$ respectively.
We note that we have
\begin{align*}
  \ProbCond{R=r, S=s}{T=t} &= \frac{\Prob[A_r \cap B_s \cap C_t]}{\Prob[C_t]} \\
  \ProbCond{R=r}{T=t} &= \frac{\Prob[A_r \cap C_t]}{\Prob[C_t]} \\
  \ProbCond{S=r}{T=t} &= \frac{\Prob[B_r \cap C_t]}{\Prob[C_t]}
\end{align*}
by the definition of the conditional probability.

\TODO{definition 3+ variables case}
\TODO{mutual independence $\neq$ pairwise independence}

\subsection{Bayes' Theorem}
\label{sec:bp-bayes}
%
\begin{theorem}[Bayes' Theorem]
  Let $A$ and $B$ be two events and $\Prob[B] \neq 0$. Then:
  \[ \Prob[A|B] = \frac{\Prob[B|A] \cdot \Prob[A]}{\Prob[B]} \]
\end{theorem}
\begin{proof}
  In section~\ref{sec:bp-indep}, we showed the following relation between marginal and conditional probability:
  \[ \Prob[A,B] = \Prob[B|A] \cdot \Prob[A] = \Prob[A|B] \cdot \Prob[B] \]
  Bayes' theorem follows immediately:
  \[ \frac{\Prob[B|A] \cdot \Prob[A]}{\Prob[B]} = \Prob[A|B] \]
\end{proof}

Bayes' Theorem is fundamental to theory we will cover in the following.
$\Prob[A]$ is called \emph{prior probability} and $\Prob[A|B]$ is called \emph{posterior probability} in the Bayesian interpretation.
The names derive from the fact, that $\Prob[A]$ is known beforehand in most applications and $\Prob[A|B]$ is the degree of belief in $A$ after $B$ happened.

\section{Probability distributions}
\label{sec:bp-dist}
%
Probability distributions are templates for probability density functions
satisfying the criteria mentioned in Section~\ref{sec:bp-continuous}.
They are parameterized by one or more variables and can be continuous or discrete.

\subsection{Normal distribution}
\label{sec:bp-norm-dist}
%
\TODO{definition}
\TODO{visualization}

\subsection{Gaussian distribution}
\label{sec:bp-gaussian-dist}
%
\TODO{definition}
\TODO{visualization}

\subsection{Independent and identically distributed}
\label{sec:bp-iid}
%
\TODO{definition}
\TODO{illustrative example}

\section{Graphical models}
\label{sec:bp-graphical-models}
%
\TODO{Show symmetry, decomposition, weak union and contraction, via Dawid et al.}

\section{Example: Polynomial curve fitting problem}
\label{sec:bp-curve-fitting}
%
\subsection{The problem}
%
\TODO{visualization}

In the following, we introduce the curve fitting problem similar to \cite[p.~4~ff.]{Bishop}.
The problem is defined as follows:

\begin{problem}[Polynomial curve fitting problem]
  Consider a polynomial of arbitrary degree.

  \begin{description}
  \item{Given}
  $x = (x_1, \ldots, x_n)^N$ as a vector of $N$ x-values and
  $t = (t_1, \ldots, t_n)^N$ as the corresponding y-values drawn from the polynomial.
  Furthermore let $E(w)$ be an error function for given polynomial coefficients $w$.

  \item{Find}
  a polynomial with coefficients $w$ which approximates values $t$ minimizing $E(w)$.
  \end{description}
\end{problem}

The degree of the polynomial is unknown on purpose.
\emph{Model selection} is a branch of Machine Learning dedicated to finding appropriate models for given problems.
So for polynomial degree choice for our curve fitting problem, we refer to research literature in Model Selection. \TODO{provide useful references for Curve Fitting}
Popular error functions include
\begin{align}
  E(w) &= \frac12 \sum_{n=1}^N \left(y(x_n, w) - t_n\right)^2 \tag{Mean squared error, MSE} \\
  E(w) &= \sqrt{\frac{1}{N} \sum_{n=1}^N (y(x_n, w) - t_n)^2} \tag{Root mean square, RMS} \\
  E(w) &= \frac1N \sum_{n=1}^N (y(x_n, w) - t_n)              \tag{Mean signed deviation, MSD}
\end{align}

\subsection{Overfitting}
\label{sec:bp-overfitting}
%
\index{Training data}
\index{Validation data}
\index{Test data}
\index{Overfitting}
Machine Learning distinguishes between a \emph{training} and \emph{validation} dataset as input.
It uses the training set to learn which output is desired for some given input.
Therefore all elements of the training set are labelled such that the error in the output can be quantified.
\emph{Overfitting} describes the situation, when the learning algorithm approximates the output with little error,
but input from the validation set (which contains different inputs) is computed with high error.
So the algorithm perfectly adapted itself to recognize the training data, but performs badly for any other input.

\TODO{visualization}

\subsection{Regularization as countermeasure}
\[ E(w) = \frac12 \sum_{n=1}^N (y(x_n, w) - t_n)^2 + \frac{\lambda}{2} \abs{w}^2 \]

% introduce gaussian distribution to every data point

We now model the problem from a probabilistic view:

\subsection{Maximum Likelihood Estimator}
%
% In particular, explain a theoretical reason of adding the term $|w|^2$ for
% the MLE problem in terms of the Baysian theorem.
%
% The Bayesian prior is exponentially to |w|^2. This is a common criticism of the application
% of MLE to Bayesian theory. There is always the prior as assumption, which in this case has computational implications.
% Computational experiments are required to verify the use.
%
The Maximum Likelihood Estimator (MLE) is a technique to estimate the parameters of a probability distribution.
It maximizes the likelihood that the given data actually occurs.

\TODO{Illustrate that the Curve Fitting problem is considered Bayesian here}

\begin{theorem}
  Consider input data $x$, mean $\mu$ and variance $\sigma^2$:
  \[ \ln{\Prob[x | \mu, \sigma^2]} = -\frac1{2\sigma^2} \sum_{n=1}^N (x_n - \mu)^2 - \frac{N}{2} \ln{\sigma^2} - \frac{N}{2} \ln(2\pi) \]
  Then
  $\mu_{\text{ML}} = \frac{1}{N} \cdot \sum_{n=1}^N x_n$ for maximized $\mu$ and \\
    $\sigma_{\text{ML}} = \frac{1}{N}\cdot \sum_{n=1}^N (x_n - \mu_{\text{ML}})^2$ for maximized $\sigma^2$
\end{theorem}

So we want to determine the 2 parameters of a Gaussian distribution, namely $\mu$ and $\sigma^2$, in the maximum likelihood case.
We begin with $\mu$:

\begin{proof}
\begin{enumerate}
  \item Derive $\ln{\Prob[x| \mu, \sigma^2]}$ for $\mu$
    \begin{align*}
      \frac\partial{\partial \mu} \ln{\Prob[x | \mu, \sigma^2]}
      &= \frac\partial{\partial \mu} \left(-\frac{1}{2\sigma^2} \cdot \sum_{n=1}^N (x_n - \mu)^2 - \frac{N}{2} \ln{\sigma^2} - \frac{N}{2} \ln(2\pi)\right) \\
      &= \frac\partial{\partial \mu} \left(-\frac{1}{2\sigma^2} \cdot \sum_{n=1}^N (x_n^2 - 2 x_n \mu + \mu^2) - \frac{N}{2} \ln{\sigma^2} - \frac{N}{2} \ln(2\pi)\right) \\
      &= -\frac{1}{2\sigma^2} \cdot \sum_{n=1}^N (-2x_n + 2\mu) \\
      &= -\frac1{\sigma^2} \cdot \sum_{n=1}^N (\mu - x_n)
    \end{align*}
  \item Set result zero
    \[ 0 = -\frac{1}{\sigma^2} \cdot \sum_{n=1}^N (\mu - x_n) = \sum_{n=1}^N (\mu - x_n) = N \cdot \mu - \sum_{n=1}^N x_n \]
    \[ \implies \mu_{\text{ML}} = \frac1N \cdot \sum_{n=1}^N x_n \qquad \text{commonly called \enquote{sample mean}} \]
\end{enumerate}
\end{proof}

We continue with $\sigma^2$ and use the same approach:

\begin{proof}
  \begin{enumerate}
  \item Derive $\ln{\Prob[x | \mu, \sigma^2]}$ for $\sigma^2$
    \begin{align*}
      \frac{\partial}{\partial \sigma^2} \ln{\Prob[x | \mu, \sigma^2]}
      &= \frac{\partial}{\partial \sigma^2} \left(-\frac{1}{2\sigma^2} \cdot \sum_{n=1}^N (x_n - \mu)^2 - \frac{N}2 \ln{\sigma^2} - \frac{N}2 \ln(2\pi)\right) \\
      &= \frac{1}{2\sigma^4} \cdot \sum_{n=1}^N (x_n - \mu)^2 - \frac{N}{2} \cdot \frac{1}{\sigma^2} \\
      &= \frac{1}{2\sigma^2} \left(\frac{1}{\sigma^2} \cdot \sum_{n=1}^N (x_n - \mu)^2 - N\right)
    \end{align*}
  \item Set result zero
    \begin{align*}
      0 &= \frac{1}{2\sigma^2} \left(\frac{1}{\sigma^2} \cdot \sum_{n=1}^N (x_n - \mu)^2 - N\right) \\
      N \cdot \sigma^2 &= \sum_{n=1}^N (x_n - \mu)^2 \\
      \sigma^2_{\text{ML}} &= \frac{1}{N} \cdot \sum_{n=1}^N (x_n - \mu)^2 \qquad \text{commonly called \enquote{sample variance}}
    \end{align*}
  \end{enumerate}
\end{proof}

And now we derive the precision parameter $\beta$ in the maximum likelihood case:

\begin{theorem}
  Given
  \[ \ln{\Prob[t | x, w, \beta]} = -\frac{\beta}{2} \cdot \sum_{n-1}^N \left(y(x_n,w) - t_n\right)^2 + \frac{N}2 \ln{\beta} - \frac{N}{2} \ln(2\pi) \]
  then find
  \[ \frac{1}{\beta_{\text{ML}}} = \frac{1}{N} \cdot \sum_{n=1}^N (y(x_n, w_{\text{ML}}) - t_n)^2 \] by maximizing $\beta$
\end{theorem}

\begin{proof}
\begin{enumerate}
  \item Derive $\ln{\Prob[t | x,w,\beta]}$ with $\beta$
    \begin{align*}
      \frac{\partial}{\partial \beta} \ln{\Prob[t | x,w,\beta]}
      &= \frac{\partial}{\partial \beta} \left(-\frac{\beta}{2} \sum_{n=1}^N (y(x_n,w) - t_n)^2 + \frac{N}{2} \ln\beta - \frac{N}{2} \ln(2\pi)\right) \\
      &= -\frac12 \cdot \sum_{n=1}^N \left(y(x_n,w) - t_n\right)^2 + \frac{N}{2} \cdot \frac1\beta
    \end{align*}
  \item Set result zero
    \begin{align*}
      0 &= -\frac{1}{2} \cdot \sum_{n=1}^N \left(y(x_n,w) - t_n\right)^2 + \frac{N}{2\beta} \\
      \frac{N}{\beta} &= \sum_{n=1}^N \left(y(x_n,w) - t_n\right)^2 \\
      \frac{1}{\beta_{\text{ML}}} &= \frac{1}{N} \cdot \sum_{n=1}^N (y(x_n,w) - t_n)^2
    \end{align*}
\end{enumerate}
\end{proof}







The maximum of the logarithm of an expression corresponds to the minimum of the negative logarithm of the same expression. \TODO{so why do we minimize and not maximize?}

\begin{align}
  -\log{\Prob[\omega|x,t,\alpha,\beta]}
    &\propto -\log\left[\Prob[t|x,\omega,\beta] \cdot \Prob[\omega|\alpha]\right] \\
\intertext{due to proportionality, $\exists c \in \mathbb R$ such that} \\
    &= -\log{\Prob[t|x,\omega,\beta]} - \log\Prob[\omega|\alpha] - \log{c} \\
\intertext{insert formula Bishop 1.62} \\
    &= \frac\beta2 \sum_{n=1}^N \left(y(x_n,\omega)-t_n\right)^2 - \frac{N}{2} \ln\beta + \frac{N}{2} \ln{2\pi} - \log\left(\left(\frac{\alpha}{2\pi}\right)^{\frac{M+1}{2}} \cdot \exp\left(-\frac{\alpha}{2} \omega^\transpose \omega\right)\right) - \log{c} \\
    &= \frac\beta2 \sum_{n=1}^N \left(y(x_n,\omega)-t_n\right)^2 - \frac{N}{2} \ln\beta + \frac{N}{2} \ln{2\pi} - \frac{M+1}{2} \log\alpha + \frac{M+1}{2} \log{2\pi} + \frac{\alpha}{2} \omega^\transpose \omega - \log{c}
\end{align}

Let $f$ be any function with a minimum. Then $\argmin_{\omega} f(\omega) = \argmin_{\omega} c \cdot f(\omega) + a$ for any $c, a \in \mathbb R$.
This applies also to our case:

\begin{align}
  \argmin_{\omega} -\log{\Prob[\omega|x,t,\alpha,\beta]}
    &= \argmin_{\omega} \left(\frac\beta2 \cdot \sum_{n=1}^N \left(y(x_n, \omega) - t_n\right)^2 + \frac\alpha2 \omega^\transpose \omega\right) \\
    &= \argmin_{\omega} \beta\left(\frac12 \sum_{n=1}^N \left(y(x_n, \omega) - t_n\right)^2 + \frac{\frac\alpha\beta}{2} \omega^\transpose \omega^\transpose\right) \\
    &= \argmin_{\omega} \frac12 \sum_{n=1}^N \left(y(x_n, \omega) - t_n\right)^2 + \frac{\frac\alpha\beta}{2} \omega^\transpose \omega^\transpose \\
\end{align}

Hence, the coefficients maximizing the probability that the coefficients correspond to our model parameters ($x, t, \alpha, \beta$) are given in the last line.
Considering we determine the best coefficients by minimizing the error function, it is justified to consider these coefficients as optimum. Let $\lambda = \frac{\alpha}{\beta}$, then \dots

\[ \tilde{E}(\omega) = \frac12 \sum_{n=1}^N \left(y(x_n, \omega) - t_n^2\right)^2 + \frac{\lambda}{2} \|\omega\|^2 \]

