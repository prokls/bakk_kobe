\chapter{Bayesian theory}
\section{Probability theory and statistics}
\subsection[Sigma-algebra]{$\boldsymbol\sigma$-algebra}
%
The following mathematical object is necessary in order to define probability properly:

\begin{definition}
  Let $X$ be a set. A $\sigma$-algebra is a set $Y$ of subsets of $X$ satisfying:
  \begin{enumerate}
    \item $\emptyset \in Y$
    \item $Z \in Y \implies Z^C \in Y$ where $Z^C$ denotes the complement of $Z$, $X \setminus Z$.
    \item $\left(\bigcup_{i=1}^n Z_i\right) \in Y$ where $n \in \Nat$ and $Z_i \in Y$ where $1 \leq i \leq n$.
  \end{enumerate}
\end{definition}
\begin{example}
  Let $X \coloneqq \set{a, b, c, d}$ and $Z \coloneqq \set{\set{a}}$. We extend $Z$ to a $\sigma$-algebra $Y$:
  \[ Y = \set{\set{}, \set{a,b,c,d}, \set{a}, \set{b,c,d}} \]
\end{example}

\subsection{Basic definitions}
\label{sec:bp-def}
%
Probability theory is concerned with random experiments and random phenomena. Probability in its basic form is the fraction of events with a certain outcome to the total number of events.
%
\begin{definition}
  \index{Probability space}
  \index{Event}
  \index{Random variable}
  A \emph{probability space} $(\Omega, \mathcal A, P)$ denotes
    the set of possible outcomes, a set of events, and a map from an element of $\mathcal A$ to a real value in $[0,1]$ respectively.
  An \emph{event} is an arbitrary set of elements in $\Omega$ satisfying the conditions of a $\sigma$-algebra.
  A \emph{random variable} can realize one of the values in $\mathcal A$.
\end{definition}
\begin{example}
  We toss a coin two times with possible outcomes heads (\textit h) or tails (\textit t). Then $\Omega = \set{h,t}$ and $\mathcal A = \set{(h, h), (h, t), (t, h), (t, t)}$.
  Let $R$ be our random variable. Our first experiment yields $(h, t)$ as result. Our second experiment yields $(h, h)$.
  Then we denote $\Prob[R=(h,t)] \coloneqq 0.5$, $\Prob[R=(h,h)] \coloneqq 0.5$, $\Prob[R=(t,h)] \coloneqq 0$, and $\Prob[R=(t,t)] \coloneqq 0$.
\end{example}

\subsection{Average Value and Expected Value}
\label{sec:bp-ae}
%
The \emph{average value} $\E[R]$ of a random variable $R$ is defined as,
\begin{align}
  % what is the sum of a set? $a$ is a set, right? Yes, but the quanitification is part of measure theory. This is not covered here.
  \E[R] &\coloneqq \frac{1}{\card{\mathcal A}} \sum_{a \in \mathcal A} a
\end{align}
If all outcomes in the probability space are considered, we call $\E[R]$ the \emph{population mean} $\mu$,
also denoting the expected value of random variable $R$.
\begin{align}
  \E[R] &\coloneqq \mu = \sum_{a \in \mathcal A} \Prob[R=a] \cdot a
\end{align}
So, average and expected values are only defined for numeric events.
Our example event $(h,t)$ does not satisfy this property.
Thus, no example is given.

Let $R$ and $S$ be two random variables and $c \in \mathbb R$.
The following properties are satisfied:
\begin{align}
  \E[c]     &\coloneqq c \\
  \E[R + c] &\coloneqq \sum_{a \in \mathcal A} \left(\Prob[R=a] \cdot (a + c)\right) \notag\\
            &= \sum_{a \in \mathcal A} \left(\Prob[R=a] \cdot a\right) + \sum_{a \in \mathcal A} \left(\Prob[R=a] \cdot c\right) \notag\\
            &= \E[R] + c \cdot \sum_{a \in \mathcal A} \Prob[R=a] \notag\\
            &= \E[R] + c \cdot 1 = \E[R] + c \\
  \E[R + S] &\coloneqq \sum_{a \in (\mathcal A_R \cup \mathcal A_S)}
              \begin{cases}
                \Prob[R=a] \cdot a & \text{if } a \in A_R \\
                \Prob[S=a] \cdot a & \text{if } a \in A_S
              \end{cases}\notag\\
            &= \sum_{a \in \mathcal A_R} \Prob[R=a] \cdot a + \sum_{a \in \mathcal A_S} \Prob[S=a] \cdot a \notag\\
            &= \E[R] + \E[S] \\
  \E[c \cdot R] &\coloneqq \sum_{a \in \mathcal A} \Prob[R=a] \cdot (c \cdot a) \notag\\
            &= c \cdot \sum_{a \in \mathcal A} \Prob[R=a] \cdot a = c \cdot \E[R]
\end{align}

\subsection{Continuous probability model}
\label{sec:bp-continuous}
%
Let $f$ be a continuous function defined in $(-\infty, \infty) \subseteq \mathbb R$.
If $f$ satisfies the properties \ref{prop:pdf1} and \ref{prop:pdf2}, $f$ is called a Probability Density Function (PDF):
\begin{align}
  f(x) &\geq 0 \qquad \forall x \in (-\infty, \infty) \label{prop:pdf1} \\
     1 &= \int_{-\infty}^{\infty} f(x) \, dx \label{prop:pdf2}
\end{align}
\begin{example}
  \[ f(x \; | \; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2} } \; e^{ -\frac{(x-\mu)^2}{2\sigma^2} } \]
  The normal distribution function depending on parameters $\mu$ and $\sigma^2$ is one example for a probability density function.
  The function is introduced in Section~\ref{sec:bp-norm-dist} in detail.
\end{example}
%
\emph{Probability} is defined as follows:
\begin{align}
  \Prob[a \leq R \leq b] &\coloneqq \int_a^b f(x) \, dx
\end{align}

% TODO: added from another section. is it consistent here?
Probabilities are linear. Hence
\begin{align}
  \Prob\left[\bigcup A\right] &= \sum \Prob\left[A\right] \label{eq:prob-linear}
\end{align}
%
\index{Expected value}
The \emph{expected value} in the continuous model is defined as,
%
\begin{align}
  \E[R] &\coloneqq \int_{-\infty}^\infty \left(\Prob[R=x] \cdot x\right) \, dx = \int_{\mathbb R} \left(\Prob[R=x] \cdot x\right) \, dx
\end{align}
%
Let $R$ and $S$ be two random variables and $c \in \mathbb R$.
The expected value satisfies:
%
\begin{align}
  \E[c]     &\coloneqq c \label{eq:Ec} \\
  \E[R + c] &\coloneqq \int_{\mathbb R} \Prob[R=x] \cdot (x + c) \, dx \notag\\
            &= \int_{\mathbb R} \left(\Prob[R=x] \cdot x + \Prob[R=x] \cdot c\right) \, dx \notag\\
            &= \int_{\mathbb R} \Prob[R=x] \cdot x \, dx + c \cdot \int_{\mathbb R} \Prob[R=x] \, dx \notag\\
            &= \E[R] + c \cdot 1 = \E[R] + c \\
  \E[R + S] &\coloneqq \int_{\mathbb R} \left(\Prob[R=x] \cdot x + \Prob[S=x] \cdot x\right) \, dx \notag\\
            &= \int_{\mathbb R} \Prob[R=x] \cdot x \, dx + \int_{\mathbb R} \Prob[S=x] \cdot x \, dx \notag\\
            &= \E[R] + \E[S] \label{eq:Evv} \\
  \E[c \cdot X] &\coloneqq \int_{\mathbb R} \Prob[R=x] \cdot (x \cdot c) \, dx \notag\\
            &= c \cdot \int_{\mathbb R} \Prob[R=x] \cdot x \, dx
            = c \cdot \E[X] \label{eq:Ecv}
\end{align}
%
Because the expected value operator $\E$ satisfies the same properties in the discrete and continuous case,
we combine those cases and denote $\E[R]$ for both cases with a random variable $R$.

\subsection{Variance and standard deviation}
\label{sec:bp-var-sd}
%
\index{Variance}
\emph{Variance} quantifies how strong values are spread out from the expected value $\E[R]$:
\[ \sigma^2 \coloneqq \E\left[(R - \E[R])^2\right] \]
Considering the entire population, the variance can also quantify over the population mean $\mu$:
\[ \sigma^2 = \Var[R] \coloneqq \E[(R - \mu)^2] \]
In the discrete case, this is equivalent to,
\[ \Var[R] = \E\left[\sum_{a \in \mathcal A} \left(\Prob[R=a] \cdot (a - \mu)\right)^2\right] \]
and in the continuous case, we have:
\[ \Var[R] = \E\left[\int_{\mathbb R} \left(\Prob[R=x] \cdot (x + \mu)\right)^2 \, dx\right] \]
The \emph{standard deviation} is defined as its second root:
\[ \sd = \sqrt{\Var[R]} \]

\subsection{Covariance}
\label{sec:covar}
%
\index{covariance}
\emph{Covariance} measures the joint variability of two given random variables.
It is defined as:
\begin{align}
  \Cov[X,Y]
    &= \E[(X - \E[X])(Y - \E[Y])] \label{eq:cov}\\
    &= \E[XY - Y\cdot\E[X] - X\cdot\E[Y] + \E[X]\E[Y]] \notag\\
    &= \E[XY] - \E[X]\cdot\E[Y] \label{eq:cov-exy}
\end{align}
%
If $X$ and $Y$ are independent (compare with Section~\ref{sec:bp-indep}), then the covariance is zero.
\begin{align}
  \Cov[X,Y] &= \E[XY] - \E[X] \cdot \E[Y] = \E[X] \cdot \E[Y] - \E[X] \cdot \E[Y] = 0 \label{eq:CovXY0}
\end{align}
We will exploit the following properties:
\begin{align}
  \Cov[X,Y] &= \E[(X - \E[X])(Y - \E[Y])] = \E[(Y - \E[Y])(X - \E[X])] = \Cov[Y,X] \\
  \Cov[X,X] &= \E[(X - \E[X])^2] = \Var[X]
\end{align}
%
Let $X$ be a set of $n$ independent variables $X_{1 \leq i \leq n}$.
\begin{align}
  \Var\left[\sum_{i=1}^n X_i\right]
    &= \E\left[\left(\sum_{i=1}^n X_i - \E\left[\sum_{i=1}^n X_i\right]\right)^2\right] \notag \\
    &= \E\left[\left(\sum_{i=1}^n \left(X_i - \E\left[X_i\right]\right)\right)^2\right] \notag \\
    &= \E\left[\sum_{i=1}^n \left(X_i - \E[X_i]\right) \cdot \sum_{j=1}^n \left(X_i - \E[X_i]\right)\right] \notag \\
    &= \E\left[\sum_{j=1}^n \left(\sum_{i=1}^n (X_i - \E[X_i])\right) (X_j - \E[X_j]) \right] \notag \\
    &= \E\left[\sum_{i,j \in [1,n]} \left(X_i - \E[X_i]\right) \left(X_j - \E[X_j]\right)\right] \notag \\
    &= \sum_{i,j \in [1,n]} \E[(X_i - \E[X_i])(X_j - \E[X_j])] \notag \\
    &= \sum_{i,j \in [1,n]} \Cov[X_i, X_j] \notag \\
    &= \sum_{\substack{i \neq j \\ i,j \in [1,n]}} \Cov[X_i, X_i] + \sum_{\substack{i \neq j \\ i,j \in [1,n]}} \Cov[X_i, X_j] \notag \\
    &= \sum_{i=1}^n \Var[X_i] + \sum_{\substack{i \neq j \\ i,j \in [1,n]}} \Cov[X_i, X_j] \notag \\
    &= \sum_{i=1}^n \Var[X_i] + 2 \cdot \sum_{1 \leq i < j \leq n} \Cov[X_i, X_j] \label{eq:sumVarCov}
\end{align}

\subsection{Law of Large Numbers}
\label{sec:bp-lln}
%
The Law of Large Numbers stresses the practical importance of the expected value.
\begin{theorem}[Law of Large Numbers]\label{thm:lln}
  First, we define the notion of the average value over a sample $A_i$ of size $n$:
  \[ \avg{R}_n \coloneqq \frac1n \sum_{i=0}^n A_i \]
  Then, the Law of Large Numbers states that,
  \[ \lim_{n\to\infty} \avg{R}_n = \E[R] \]
\end{theorem}

In order to prove this theorem, we use Chebyshev's Inequality, the Weak Law of Large numbers,
Borel-Cantelli Lemma and the Strong Law of Large Numbers. The latter is considered equivalent
to the Law of Large numbers~\footnote{
  Depending on your requirements of certainty, the Weak Law of Large numbers might be already
  considered equivalent to the Law of Large Numbers (Theorem~\ref{thm:lln}), but we look
  for the Strong Law of Large Numbers in this thesis.
}. Our proof structure is based on Craig A. Tracy's~\cite{cnfgen}\footnote{
  Please recognize that there is a small typographical error on page~3.
  \enquote{$S_n(\omega) = 1$ for every $n$} should be \enquote{$X_n(\omega) = 1$ for every $n$}.
}.

\subsubsection{Chebyshev's Inequality}
\label{sec:bp-chebyshev}
%
Consider the continuous case.
Let $R$ be a random variable, $f$ be a PDF over $R$, $|\cdot|$ be a norm,
$a \in \mathbb R_{\geq 0}$, $p \in \mathbb N$ and let $\E[\norm R^p]$ be defined as follows:
\begin{align*}
  \E[\norm{R}^p]
          = \int\limits_{\mathbb R} \norm{x}^p \cdot f(x) \, dx 
          \geq \int\limits_{\norm{x} \geq a} x^p \cdot f(x) \, dx 
          \geq a^p \int\limits_{\norm{x} \geq a} f(x) \, dx 
          = a^p \cdot \Prob[\norm{R} \geq a]
\end{align*}
The discrete case follows immediately.
This concludes the correctness of the following theorem:
\begin{theorem}[Chebyshev's Inequality Theorem]\label{thm:chebyshev}
  Let $R$ be a random variable, $\norm{\cdot}$ be a norm, $a \in \mathbb R_{\geq 0}$ and $p \in \mathbb N$ is arbitrary.
  Assume $\E[\norm R^p] < \infty$.
  Then it holds that,
  \[ \Prob[\norm{R} \geq a] \leq \frac{1}{a^p} \E[\norm{R}^p] \]
\end{theorem}

\subsubsection{Weak Law of Large Numbers}
\label{sec:bp-weak-law}
%
The next theorem is called Weak Law of Large Numbers.
\begin{theorem}[Weak Law of Large Numbers, Bernoulli's Theorem]\label{thm:weak-lln}
  Let $R_i$ be a sequence of independent and identically distributed random variables
  (see section~\ref{sec:bp-iid} for a definition of i.i.d.)
  with common mean $\mean$ and variance $\var$. Let
  \[
      S_n \coloneqq \sum_{i=1}^n R_i \hspace{50pt}
      T_n \coloneqq \frac{S_n}{n} - \mean
  \]
  Then for any $\varepsilon > 0$,
  \[ \lim_{n\to\infty} \Prob[\norm{T_n} \geq \varepsilon] = 0 \]
\end{theorem}

\begin{proof}
  First, we determine the expected values.
  \begin{align}
      \E[S_n] &= \E\left[\sum_{i=1}^n R_i\right] = \sum_{i=1}^n \E[R_i] = \sum_{i=1}^n \mu = n \cdot \mu & \eqref{eq:Evv} \label{eq:ES_n} \\
%      \E[T_n] &= \E\left[\frac{1}{n}\left(R_1 + R_2 + \ldots + R_n\right) - \mu\right] \notag\\
%              &= \frac1n \left(\E[R_1] + \E[R_2] + \ldots + \E[R_n]\right) - \E[\mu] & \eqref{eq:Ecv} \notag\\
%              &= \frac{n \cdot \mu}{n} - \mu = 0 \label{eq:tn0}
      \E[T_n] &= \E\left[\frac{S_n}{n} - \mu\right] = \frac{\E\left[S_n\right]}{\E\left[n\right]} - \E\left[\mu\right] & \eqref{eq:Evv} \notag\\
              &= \frac{n \cdot \mu}{n} - \mu = 0  & \eqref{eq:ES_n}, \eqref{eq:Ecv} \label{eq:tn0}
  \end{align}
  We also need a result regarding the variance.
  In the continuous and discrete case, it holds that $a^2 \cdot \Var[X] = \Var[a \cdot X]$:
  \begin{align}
    a^2 \cdot \Var[X] &= a^2 \cdot \int (x - \mu)^2 \cdot f(x) \, dx = \int (x \cdot a - \mu \cdot a)^2 \cdot f(x) \, dx \notag \\
    a^2 \cdot \Var[X] &= a^2 \cdot \sum_{i=1}^n p_i \cdot (x_i - \mu)^2 = \sum_{i=1}^n p_i \cdot a^2 \cdot (x_i - \mu)^2 = \sum_{i=1}^n p_i \cdot (x_i \cdot a - \mu \cdot a)^2 \label{eq:a2V}
  \end{align}
  The relation $\E[\norm{X}^2] = \Var[X] + \E[X]^2$ holds as well,
  \begin{align}
    \Var[X] &= \E[(X - \mu)^2] = \E[\norm{X}^2] - \E[2X\mu] + \E[\mu^2] \notag & \eqref{eq:Evv} \\
            &= \E[\norm X^2] - 2 \cdot \E\left[X \cdot \E[X]\right] + \E\left[\E[X]^2\right] & \eqref{eq:Ecv} \notag\\
            &= \E[\norm X^2] - 2 \cdot \E[X]^2 + \E[X]^2 & \eqref{eq:Ecv} \notag\\
            &= \E[\norm X^2] - \E[X]^2 \label{eq:VXisEX2-EX2}
  \end{align}
  We use this result to prove $\Var[T_n] = \frac{\sigma^2}{n}$.
  \begin{align}
    \Var[T_n] &= \E[\norm{T_n}^2] - \E[T_n]^2 = \E[\norm{T_n}^2] - 0^2 = \E[\norm{T_n}^2] & \eqref{eq:VXisEX2-EX2} \eqref{eq:tn0} \notag\\
% TODO: \norm disappears here. Is is imprecise?
              &= \E\left[\left(\frac{S_n}{n} - \mu\right)^2\right] = \E\left[\left(\frac{S_n}{n}\right)^2 - 2\frac{S_n}{n}\mu + \mu^2\right] \notag \\
              &= \E\left[\left(\frac{S_n}{n}\right)^2\right] - 2\cdot\E\left[\frac{\mu}{n} S_n\right] + \E\left[\mu^2\right] & \eqref{eq:Evv}\eqref{eq:Ecv} \notag\\
              &= \Var\left[\frac{S_n}{n}\right] + \E\left[\frac{S_n}{n}\right]^2 - 2\mu \cdot \E\left[\frac{S_n}{n}\right] + \mu^2 & \eqref{eq:VXisEX2-EX2}\eqref{eq:Ec} \notag \\
              &= \frac1{n^2} \cdot \Var\left[S_n\right] + \left(\frac1{n} \cdot \E\left[S_n\right]\right)^2 - \frac{2\mu}{n} \cdot (n \cdot \mu) + \mu^2 & \eqref{eq:Ecv}\eqref{eq:a2V} \notag \\
              &= \frac1{n^2} \cdot \left(\Var\left[S_n\right] + \E[S_n]^2\right) - 2\mu^2 + \mu^2 \notag \\
              &= \frac{1}{n^2} \left(\Var\left[\sum_{i=1}^n R_i\right] + \E[S_n]^2\right) - \mu^2 \notag\\
              &= \frac{1}{n^2} \left(\sum_{i=1}^n \Var[R_i] + 2\sum_{\substack{i<j \\ i,j=1}}^n \Cov[R_i,R_j] + \E[S_n]^2\right) - \mu^2 & \eqref{eq:sumVarCov} \notag\\
              &= \frac{1}{n^2} \left(\sum_{i=1}^n \sigma^2 + 0 + (n \cdot \mu)^2\right) - \mu^2 & \eqref{eq:ES_n} & \eqref{eq:CovXY0} \notag\\
              &= \frac{1}{n^2} \left(n \cdot \sigma^2 + n^2 \cdot \mu^2\right) - \mu^2 = \frac{\sigma^2}{n} + \mu^2 - \mu^2 = \frac{\sigma^2}{n}
  \end{align}
\end{proof}
%
%% REMARK: Yes, this is a quote from Wikipedia. Yes, it is true. But it is too short, because we didn't show that the covariance is zero and whether they are uncorrelated or not. So I prefer the approach above
%\index{Bienaym\'e formula}
% This proves the so-called Bienaym\'e formula.
% A shorter approach uses the definition of the mean:
% \[ \Var[\avg{T_n}] = \Var\left(\frac1n \sum_{i=1}^n X_i\right) = \frac1{n^2} \sum_{i=1}^n \Var[X_i] = \frac{\sigma^2}{n} \]

Furthermore, the following equation holds:
\begin{align}
    \Var[\norm{T_n}] &= \E\left[(T_n - \E[T_n])^2\right] = \E\left[(T_n - 0)^2\right] = \E\left[\norm{T_n}^2\right] \label{eq:EVT_n}
\end{align}
%
Now we can apply Chebyshev's Inequality Theorem ($R = \norm{T_n}$, $a = \varepsilon \in \mathbb R, p = 2$):
\begin{align}
  \Prob[\abs{T_n} \geq \varepsilon]
    &\leq \frac{1}{\varepsilon^2} \E\left[\abs{T_n}^2\right]
    = \frac{1}{\varepsilon^2} \Var\left[\abs{T_n}\right]
    = \frac{1}{\varepsilon^2} \frac{\sigma^2}{n}
    & \eqref{eq:EVT_n} \label{eq:cheby-applied}
\end{align}
For any $\varepsilon > 0$ with $n \to \infty$, it holds that
\[ \Prob[\abs{T_n} \geq \varepsilon] \to 0 \]
\[
    \Leftrightarrow \forall \varepsilon > 0:
    \lim_{n\to\infty} \Prob[\abs{T_n} \geq \varepsilon] = 0
\]

This concludes the proof of the Weak Law of Large Numbers.
In order to finish our proof of the Strong Law of Large numbers,
we will use the Borel-Cantelli Lemma, an important result of measure theory.

\subsubsection{Borel-Cantelli Lemma}
\label{sec:bp-borel-cantelli}
%
\begin{lemma}[Borel-Cantelli Lemma]\label{lemma:bcl}
  Let $R_i$ with $1 \leq i < \infty$ be a sequence of events.
  Assume the sum of these probabilities is finite, then it holds that:
  \begin{align}
    \sum_{i=1}^\infty \Prob[R_i] < \infty
    &\implies
    \Prob\left(\limsup\limits_{i\to\infty} R_i\right) = 0
  \end{align}
  So the probability, that the occuring event is an event which occurs infinitely often, is 0.
\end{lemma}

Please consider, that the limsup is defined as,
\begin{align}
  \limsup\limits_{i\to\infty} R_i &\coloneqq \bigcap_{j=1}^\infty \bigcup_{i \geq j}^\infty R_i \label{eq:limsup-char}
\end{align}
%Equivalently, we can also characterize it the following way:
%\begin{definition}
%  Let $S_i = \bigcup_{j \geq i}^\infty R_j$. Then
%  \[
%    \forall r \, \exists k \in \mathbb N \, \forall l > k: \:
%    r \in \left(\lim_{i\to\infty} S_i\right) \Leftrightarrow r \in S_k
%  \]
%\end{definition}
%
%To prove this theorem, we consider the following (in)equalities:
%\begin{align}
%  \Prob\left(\limsup\limits_{n\to\infty} R_n\right)
%      &= \Prob\left(\bigcap_{l=1}^\infty \bigcup_{n=l}^\infty R_n\right) \notag\\
%      &\leq \inf_{l \geq 1} \Prob\left(\bigcup_{n=l}^\infty R_n\right) \notag\\
%      &\leq \inf_{l \geq 1} \sum_{n=l}^\infty \Prob\left(R_n\right) \notag\\
%      &= 0  \label{eq:limsup-0-result}
%\end{align}
%
%To understand these lines, you need to recognize that $\bigcap_{l=1}^\infty$
%represents the set-equivalent of $\inf_{l\geq1}$ for one set of numeric values.
%However, the infimum (unlike the result of an intersection) must not be an element
%of the set itself, hence yielding the inequality. This can be easily determined by
%the equivalence of the characterizations shown above. Furthermore, probabilities satisfy
%linearity (the probability of one of the events $A$, $B$, and $C$ equals the probability
%of the union of $A$, $B$, and $C$). Finally, $\bigcup_{n=l}^\infty$ represents
%the set-equivalent of $\sum_{n=l}^\infty$.

The condition requires, that $\sum_{i=1}^\infty \Prob[R_i] < \infty$.
This statement is equivalent to
\begin{align}
  \inf_{j \geq 1} \sum_{i=j}^\infty \Prob[R_i] &= 0  \label{eq:inf0}
\end{align}

We can make our final conclusion to prove the Borel-Cantelli Lemma:
\begin{align}
  \Prob\left[\limsup_{i\to\infty} R_i\right]
    &= \Prob\left[\bigcap_{j=1}^\infty \bigcup_{i=j}^\infty R_i\right] & \eqref{eq:limsup-char} \notag\\
    &\leq \inf_{j \geq 1} \Prob\left[\bigcup_{i=j}^\infty R_i\right] & \eqref{eq:prob-linear}\notag\\
    &\leq \inf_{j \geq 1} \sum_{i=j}^\infty \Prob\left[R_i\right] \notag\\
    &= 0 & \eqref{eq:inf0}
\end{align}
% $\bigcap_{j=1}^\infty$ roughly corresponds to the infimum.
% $\bigcup_{i=j}^\infty$ roughly corresponds to $\sum_{i=j}^\infty$.
The result of an intersection of elements creates a set, which
is an actual subset in any of these sets. However, the infimum
is not necessarily an element of the set. Hence, an inequality
is introduced in the second line.

\subsubsection{Strong Law of Large Numbers}
\label{sec:bp-strong-law}
%
\begin{theorem}[Strong Law of Large Numbers]\label{thm:lln}
  Assume the definitions of Theorem~\ref{thm:weak-lln}.
  Therefore, $R_1, R_2, \ldots$ is an infinite sequence of independent random variables
  with a common distribution ($\mu = \E[R_j]$, $\sigma^2 = \Var[R_j]$). $S_n$ and $T_n$
  are defined. Now consider event $\mathcal E$:
  \[
    \mathcal E = \set{
      \omega \in \Omega: \lim_{n\to\infty} \frac{S_n(\omega)}{n} = \mu
    }
  \]
  Then it holds that
  \[ \Prob[\mathcal E] = 1 \]
\end{theorem}

The following proof assumes $\sigma^2 = \E[\norm{R_j}^2] < \infty$ and $\E[\norm{R_j}^4] < \infty$.
This restriction makes our proof easier, but the less restricted case $\E[\norm{R_j}] < \infty$ suffices as assumption
(but this is not proven in this thesis).

Without loss of generality we assume $\mu = 0$.
If $\mu = 0$ is not satisfied, we consider $P_j \coloneqq R_j - \mu$ instead.

Now, the proof structure works as follows: if
\[ \lim_{n\to\infty} \frac{S_n(\omega)}{n} \neq 0 \]
then $\exists \varepsilon \in \mathbb R$ with $\varepsilon > 0$ such that for infinitely many $n$
\[ \norm{\frac{S_n(\omega)}{n}} > \varepsilon \]
So to prove the theorem, we will prove that for every $\varepsilon > 0$,
\[ \Prob[\norm{S_n} > n \cdot \varepsilon \text{ infinitely often}] = 0 \]
In the following, this reveals that
\[ \Prob[\mathcal E] = \Prob\left[\frac{S_n}{n} = 0\right] = 1 \]
proving Theorem~\ref{thm:lln}.

First of all, we define
\[ A_n = \set{\omega \in \Omega: \norm{S_n} \geq n \cdot \varepsilon} \]
and look at $\Prob[A_n]$ using the Chebyshev inequality (Theorem~\ref{thm:chebyshev}) with $p=4$ and $a=n\cdot\varepsilon$:
\[ \Prob[\norm{S_n} \geq (n\cdot\varepsilon)] \leq \frac{1}{(n\cdot\varepsilon)^4} \E[\norm{S_n}^4] \]
We must determine $\E[\norm{S_n}^4]$ which equals to
\begin{align*}
  \E\left[\sum_{1 \leq i,j,k,l \leq n} R_i R_j R_k R_l\right]
    &= \E\left[\sum_{1\leq i}^n \sum_{1\leq j}^n \sum_{1\leq k}^n \sum_{1\leq l}^n R_i R_j R_k R_l\right] \notag\\
    &= \E\left[(R_1^4 + \ldots + R_1 R_n^3) + (R_2 \ldots) + (R_3 \ldots) + (R_n \ldots + R_n^4) \right]
\end{align*}
Because $\E[R_i] = 0$, we can remove all terms containing $R_j$ of degree 1 for any $j$.
These are terms of the structure (assuming $i, j, k$ and $l$ distinct),
\[
  \E[R_i^3 R_j], \,
  \E[R_i^2 R_j R_k], \,
  \E[R_i R_j R_k R_l]
\]
Remember that multiplication of expected values (compare with Equation~\ref{eq:CovXY0}) applies here.
The non-zero terms are $\E[R_i^4]$ and $\E[R_i^2 R_j^2] = \left(\E[R_i^2]\right)^2$.
Now, we need to quantify the occurences of these non-zero terms. $\E[R_i^4]$ occurs $n$ times.
Terms $\E[R_i^2 R_j^2]$ occur $3n \cdot (n-1)$ times, as there are $\frac{(n-1) \cdot n}{2}$ ways to choose 2 indices and 6 ways to find $R_i^2 R_j^2$.
In conclusion, we determined, % $\E\left[\norm{S_n}^4\right]$,
\[ \E\left[\norm{S_n}^4\right] = n \cdot \E\left[R_1^4\right] + 3n \cdot (n - 1) \cdot \sigma^4 \]
In this expression, $n$ is our only constant occuring with polynomial degree $2$. So for any $n$ sufficiently large, there exists $C \in \mathbb R$ such that
\begin{align}
  3 \sigma^4 n^2 + \left(\E\left[\norm{R_1}^4\right] - 3 \sigma^4\right) \cdot n &\leq C \cdot n^2 \label{eq:3sigman2}
\end{align}
\[ \Rightarrow \E\left(\norm{S_n}^4\right) \leq C n^2 \]

We return back to Chebyshev's inequality
\[
    \Prob[\norm{S_n} \geq (n\cdot\varepsilon)]
    \leq \frac{1}{(n\cdot\varepsilon)^4} \E[\norm{S_n}^4]
    \leq \frac{C \cdot n^2}{\varepsilon^4 \cdot n^2 \cdot n^2}
\]
It follows that, there exists some $n_0$ such that Equation~\eqref{eq:3sigman2} is satisfied.
With this approach, we skip a finite number of elements of the sum. This does not affect its convergence or divergence.
\[
    \sum_{n \geq n_0} \Prob\left[\norm{S_n} \geq n \cdot \varepsilon\right]
    \leq \sum_{n \geq n_0} \frac{C}{\varepsilon^4 n^2} < \infty
\]
Therefore, the condition to apply the Borel-Cantelli Lemma (Lemma~\ref{lemma:bcl}) are satisfied.
For every $\varepsilon > 0$ it holds that,
\[ \Prob\left[\norm{S_n} \geq n \varepsilon \text{ infinitely often}\right] = 0 \]

\subsection{Union and intersection of events}
\label{sec:bp-unin}
%
\index{event}
We denoted $\mathcal A$ as the set of events.
A element of $\mathcal A$ is called \emph{event} and satisfies the
properties of a $\sigma$-algebra.

We define the union $U$ of events $a,b \in \mathcal A$ as union of the sets; $c = a \cup b$.
The semantics following implicitly. Random variable $R$ realizes $c$ if event $a$ or $b$ or both occur.

\TODO{Better illustration, better consistency with marginalization, independence, conditional independence}

\subsection{Marginalization}
\label{sec:bp-marginalization}
%
\[ \Prob[R=r] = \sum_{s \in S} \Prob[R=r, S=s] \]

\TODO{Explanation}
\TODO{Illustrative example}

\subsection{Joint distribution}
\label{sec:bp-joint-distribution}
%
\[ \Prob[R=r, S=s] = \Prob[R=r] \cdot \Prob[S=s] \]

\TODO{Explanation}
\TODO{Illustrative example}

\subsection{Independence}
\label{sec:bp-indep}
%
\[ \Prob[R=r] = \sum_{s \in S} \Prob[R=r, S=s] \]

\TODO{Explanation}
\TODO{Illustrative example}

\subsection{Conditional independence}
\label{sec:bp-cond-indep}
%
Conditional independence concerns two or more random variables $R_i$.
Conditional dependence is given if the outcome of a random variable $R_i$ changes the probability of the outcome of a random variable $R_j$ with $i \neq j$.
Let $A$ and $B$ be two random variables and $a$ and $b$ be two distinctive outcomes. Trivially, we can first observe that:
\begin{align*}
  \Prob[A=a|A=a] &= 1 \\
  \Prob[A=b|A=a] &= 0
\end{align*}
The notation $\Prob[A=b|A=a]$ signifies the probability that the event $A=b$ occurs if event $A=a$ as condition occured. In this particular case, the probability is obviously $0$, because a random variable cannot take two values simultaneously. The first case evaluates to $1$, because if we are assured that random variable $A$ will be realized with $a$, then random variable $A$ will take value $a$ with probability $1$.

Formally, conditional independence is defined the following way: Random variables $A$ and $B$ are conditionally independent if and only if
\[ \Prob[A, B] = \Prob[A] \cdot \Prob[B] \]

An interesting question arises for three or more variables.

\TODO{definition 3+ variables case}
\TODO{mutual independence $\neq$ pairwise independence}

\subsection{Bayes' Theorem}
\label{sec:bp-bayes}
%
\begin{theorem}[Bayes' Theorem]
  Let $A$ and $B$ be two events and $\Prob[B] \neq 0$. Then:
  \[ \Prob[A|B] = \frac{\Prob[B|A] \cdot \Prob[A]}{\Prob[B]} \]
\end{theorem}
\begin{proof}
  In section~\ref{sec:bp-indep}, we showed the following relation between marginal and conditional probability:
  \[ \Prob[A,B] = \Prob[B|A] \cdot \Prob[A] = \Prob[A|B] \cdot \Prob[B] \]
  Bayes' theorem follows immediately:
  \[ \frac{\Prob[B|A] \cdot \Prob[A]}{\Prob[B]} = \Prob[A|B] \]
\end{proof}

Bayes' Theorem is fundamental to theory we will cover in the following.
$\Prob[A]$ is called \emph{prior probability} and $\Prob[A|B]$ is called \emph{posterior probability} in the Bayesian interpretation.
The names derive from the fact, that $\Prob[A]$ is known beforehand in most applications and $\Prob[A|B]$ is the degree of belief in $A$ after $B$ happened.

\section{Probability distributions}
\label{sec:bp-dist}
%
Probability distributions are templates for probability density functions
satisfying the criteria mentioned in Section~\ref{sec:bp-continuous}.
They are parameterized by one or more variables and can be continuous or discrete.

\subsection{Normal distribution}
\label{sec:bp-norm-dist}
%
\TODO{definition}
\TODO{visualization}

\subsection{Gaussian distribution}
\label{sec:bp-gaussian-dist}
%
\TODO{definition}
\TODO{visualization}

\subsection{Independent and identically distributed}
\label{sec:bp-iid}
%
\TODO{definition}
\TODO{illustrative example}

\section{Graphical models}
\label{sec:bp-graphical-models}
%
\TODO{Show symmetry, decomposition, weak union and contraction, via Dawid et al.}

\section{Example: Polynomial curve fitting problem}
\label{sec:bp-curve-fitting}
%
\subsection{The problem}
%
\TODO{visualization}

In the following, we introduce the curve fitting problem similar to \cite[p.~4~ff.]{Bishop}.
The problem is defined as follows:

\begin{problem}[Polynomial curve fitting problem]
  Consider a polynomial of arbitrary degree.

  \begin{description}
  \item{Given}
  $x = (x_1, \ldots, x_n)^N$ as a vector of $N$ x-values and
  $t = (t_1, \ldots, t_n)^N$ as the corresponding y-values drawn from the polynomial.
  Furthermore let $E(w)$ be an error function for given polynomial coefficients $w$.

  \item{Find}
  a polynomial with coefficients $w$ which approximates values $t$ minimizing $E(w)$.
  \end{description}
\end{problem}

The degree of the polynomial is unknown on purpose.
\emph{Model selection} is a branch of Machine Learning dedicated to finding appropriate models for given problems.
So for polynomial degree choice for our curve fitting problem, we refer to research literature in Model Selection. \TODO{provide useful references for Curve Fitting}
Popular error functions include
\begin{align}
  E(w) &= \frac12 \sum_{n=1}^N \left(y(x_n, w) - t_n\right)^2 \tag{Mean squared error, MSE} \\
  E(w) &= \sqrt{\frac{1}{N} \sum_{n=1}^N (y(x_n, w) - t_n)^2} \tag{Root mean square, RMS} \\
  E(w) &= \frac1N \sum_{n=1}^N (y(x_n, w) - t_n)              \tag{Mean signed deviation, MSD}
\end{align}

\subsection{Overfitting}
\label{sec:bp-overfitting}
%
\index{training data}
\index{validation data}
\index{test data}
\index{overfitting}
Machine Learning distinguishes between a \emph{training} and \emph{validation} dataset as input.
It uses the training set to learn which output is desired for some given input.
Therefore all elements of the training set are labelled such that the error in the output can be quantified.
\emph{Overfitting} describes the situation, when the learning algorithm approximates the output with little error,
but input from the validation set (which contains different inputs) is computed with high error.
So the algorithm perfectly adapted itself to recognize the training data, but performs badly for any other input.

\TODO{visualization}

\subsection{Regularization as countermeasure}
\[ E(w) = \frac12 \sum_{n=1}^N (y(x_n, w) - t_n)^2 + \frac{\lambda}{2} \abs{w}^2 \]

% introduce gaussian distribution to every data point

We now model the problem from a probabilistic view:

\subsection{Maximum Likelihood Estimator}
%
% In particular, explain a theoretical reason of adding the term $|w|^2$ for
% the MLE problem in terms of the Baysian theorem.
%
% The Bayesian prior is exponentially to |w|^2. This is a common criticism of the application
% of MLE to Bayesian theory. There is always the prior as assumption, which in this case has computational implications.
% Computational experiments are required to verify the use.
%
The Maximum Likelihood Estimator (MLE) is a technique to estimate the parameters of a probability distribution.
It maximizes the likelihood that the given data actually occurs.

\TODO{Illustrate that the Curve Fitting problem is considered Bayesian here}

\begin{theorem}
  Consider input data $x$, mean $\mu$ and variance $\sigma^2$:
  \[ \ln{\Prob[x | \mu, \sigma^2]} = -\frac1{2\sigma^2} \sum_{n=1}^N (x_n - \mu)^2 - \frac{N}{2} \ln{\sigma^2} - \frac{N}{2} \ln(2\pi) \]
  Then
  $\mu_{\text{ML}} = \frac{1}{N} \cdot \sum_{n=1}^N x_n$ for maximized $\mu$ and \\
    $\sigma_{\text{ML}} = \frac{1}{N}\cdot \sum_{n=1}^N (x_n - \mu_{\text{ML}})^2$ for maximized $\sigma^2$
\end{theorem}

So we want to determine the 2 parameters of a Gaussian distribution, namely $\mu$ and $\sigma^2$, in the maximum likelihood case.
We begin with $\mu$:

\begin{proof}
\begin{enumerate}
  \item Derive $\ln{\Prob[x| \mu, \sigma^2]}$ for $\mu$
    \begin{align*}
      \frac\partial{\partial \mu} \ln{\Prob[x | \mu, \sigma^2]}
      &= \frac\partial{\partial \mu} \left(-\frac{1}{2\sigma^2} \cdot \sum_{n=1}^N (x_n - \mu)^2 - \frac{N}{2} \ln{\sigma^2} - \frac{N}{2} \ln(2\pi)\right) \\
      &= \frac\partial{\partial \mu} \left(-\frac{1}{2\sigma^2} \cdot \sum_{n=1}^N (x_n^2 - 2 x_n \mu + \mu^2) - \frac{N}{2} \ln{\sigma^2} - \frac{N}{2} \ln(2\pi)\right) \\
      &= -\frac{1}{2\sigma^2} \cdot \sum_{n=1}^N (-2x_n + 2\mu) \\
      &= -\frac1{\sigma^2} \cdot \sum_{n=1}^N (\mu - x_n)
    \end{align*}
  \item Set result zero
    \[ 0 = -\frac{1}{\sigma^2} \cdot \sum_{n=1}^N (\mu - x_n) = \sum_{n=1}^N (\mu - x_n) = N \cdot \mu - \sum_{n=1}^N x_n \]
    \[ \implies \mu_{\text{ML}} = \frac1N \cdot \sum_{n=1}^N x_n \qquad \text{commonly called \enquote{sample mean}} \]
\end{enumerate}
\end{proof}

We continue with $\sigma^2$ and use the same approach:

\begin{proof}
  \begin{enumerate}
  \item Derive $\ln{\Prob[x | \mu, \sigma^2]}$ for $\sigma^2$
    \begin{align*}
      \frac{\partial}{\partial \sigma^2} \ln{\Prob[x | \mu, \sigma^2]}
      &= \frac{\partial}{\partial \sigma^2} \left(-\frac{1}{2\sigma^2} \cdot \sum_{n=1}^N (x_n - \mu)^2 - \frac{N}2 \ln{\sigma^2} - \frac{N}2 \ln(2\pi)\right) \\
      &= \frac{1}{2\sigma^4} \cdot \sum_{n=1}^N (x_n - \mu)^2 - \frac{N}{2} \cdot \frac{1}{\sigma^2} \\
      &= \frac{1}{2\sigma^2} \left(\frac{1}{\sigma^2} \cdot \sum_{n=1}^N (x_n - \mu)^2 - N\right)
    \end{align*}
  \item Set result zero
    \begin{align*}
      0 &= \frac{1}{2\sigma^2} \left(\frac{1}{\sigma^2} \cdot \sum_{n=1}^N (x_n - \mu)^2 - N\right) \\
      N \cdot \sigma^2 &= \sum_{n=1}^N (x_n - \mu)^2 \\
      \sigma^2_{\text{ML}} &= \frac{1}{N} \cdot \sum_{n=1}^N (x_n - \mu)^2 \qquad \text{commonly called \enquote{sample variance}}
    \end{align*}
  \end{enumerate}
\end{proof}

And now we derive the precision parameter $\beta$ in the maximum likelihood case:

\begin{theorem}
  Given
  \[ \ln{\Prob[t | x, w, \beta]} = -\frac{\beta}{2} \cdot \sum_{n-1}^N \left(y(x_n,w) - t_n\right)^2 + \frac{N}2 \ln{\beta} - \frac{N}{2} \ln(2\pi) \]
  then find
  \[ \frac{1}{\beta_{\text{ML}}} = \frac{1}{N} \cdot \sum_{n=1}^N (y(x_n, w_{\text{ML}}) - t_n)^2 \] by maximizing $\beta$
\end{theorem}

\begin{proof}
\begin{enumerate}
  \item Derive $\ln{\Prob[t | x,w,\beta]}$ with $\beta$
    \begin{align*}
      \frac{\partial}{\partial \beta} \ln{\Prob[t | x,w,\beta]}
      &= \frac{\partial}{\partial \beta} \left(-\frac{\beta}{2} \sum_{n=1}^N (y(x_n,w) - t_n)^2 + \frac{N}{2} \ln\beta - \frac{N}{2} \ln(2\pi)\right) \\
      &= -\frac12 \cdot \sum_{n=1}^N \left(y(x_n,w) - t_n\right)^2 + \frac{N}{2} \cdot \frac1\beta
    \end{align*}
  \item Set result zero
    \begin{align*}
      0 &= -\frac{1}{2} \cdot \sum_{n=1}^N \left(y(x_n,w) - t_n\right)^2 + \frac{N}{2\beta} \\
      \frac{N}{\beta} &= \sum_{n=1}^N \left(y(x_n,w) - t_n\right)^2 \\
      \frac{1}{\beta_{\text{ML}}} &= \frac{1}{N} \cdot \sum_{n=1}^N (y(x_n,w) - t_n)^2
    \end{align*}
\end{enumerate}
\end{proof}







The maximum of the logarithm of an expression corresponds to the minimum of the negative logarithm of the same expression. \TODO{so why do we minimize and not maximize?}

\begin{align}
  -\log{\Prob[\omega|x,t,\alpha,\beta]}
    &\propto -\log\left[\Prob[t|x,\omega,\beta] \cdot \Prob[\omega|\alpha]\right] \\
\intertext{due to proportionality, $\exists c \in \mathbb R$ such that} \\
    &= -\log{\Prob[t|x,\omega,\beta]} - \log\Prob[\omega|\alpha] - \log{c} \\
\intertext{insert formula Bishop 1.62} \\
    &= \frac\beta2 \sum_{n=1}^N \left(y(x_n,\omega)-t_n\right)^2 - \frac{N}{2} \ln\beta + \frac{N}{2} \ln{2\pi} - \log\left(\left(\frac{\alpha}{2\pi}\right)^{\frac{M+1}{2}} \cdot \exp\left(-\frac{\alpha}{2} \omega^\transpose \omega\right)\right) - \log{c} \\
    &= \frac\beta2 \sum_{n=1}^N \left(y(x_n,\omega)-t_n\right)^2 - \frac{N}{2} \ln\beta + \frac{N}{2} \ln{2\pi} - \frac{M+1}{2} \log\alpha + \frac{M+1}{2} \log{2\pi} + \frac{\alpha}{2} \omega^\transpose \omega - \log{c}
\end{align}

Let $f$ be any function with a minimum. Then $\argmin_{\omega} f(\omega) = \argmin_{\omega} c \cdot f(\omega) + a$ for any $c, a \in \mathbb R$.
This applies also to our case:

\begin{align}
  \argmin_{\omega} -\log{\Prob[\omega|x,t,\alpha,\beta]}
    &= \argmin_{\omega} \left(\frac\beta2 \cdot \sum_{n=1}^N \left(y(x_n, \omega) - t_n\right)^2 + \frac\alpha2 \omega^\transpose \omega\right) \\
    &= \argmin_{\omega} \beta\left(\frac12 \sum_{n=1}^N \left(y(x_n, \omega) - t_n\right)^2 + \frac{\frac\alpha\beta}{2} \omega^\transpose \omega^\transpose\right) \\
    &= \argmin_{\omega} \frac12 \sum_{n=1}^N \left(y(x_n, \omega) - t_n\right)^2 + \frac{\frac\alpha\beta}{2} \omega^\transpose \omega^\transpose \\
\end{align}

Hence, the coefficients maximizing the probability that the coefficients correspond to our model parameters ($x, t, \alpha, \beta$) are given in the last line.
Considering we determine the best coefficients by minimizing the error function, it is justified to consider these coefficients as optimum. Let $\lambda = \frac{\alpha}{\beta}$, then \dots

\[ \tilde{E}(\omega) = \frac12 \sum_{n=1}^N \left(y(x_n, \omega) - t_n^2\right)^2 + \frac{\lambda}{2} \|\omega\|^2 \]

