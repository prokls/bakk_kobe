\documentclass[a4paper,oneside]{book}
\usepackage{fontspec}
\usepackage{bm}
\usepackage[small,sf,bf]{titlesec}
\usepackage{polyglossia}\setdefaultlanguage{english}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage[autostyle=true]{csquotes}
\usepackage[backend=bibtex,natbib=true]{biblatex}
\usepackage[
    pdfauthor={Lukas Prokop},
    pdftitle={Bayesian statistics and Neural networks},
    pdfsubject={Bayesian probability theory and Neural networks applied to character recognition},
    pdfkeywords={Bayes, probability, neural network, classifier, character, unicode, kanji, mathematical expressions}
    ]{hyperref} % dep: after biblatex
\usepackage{makeidx}
\usepackage{microtype}
\usepackage{ntheorem}  % dep: before thmtools
\usepackage{thmtools}
\usepackage{xunicode}
\usepackage{xcolor}
\usepackage{metalogo}
\usepackage[fallback]{xeCJK}
\usepackage{mathtools}
\usepackage[toc,page]{appendix}
\usepackage{minted}
\usepackage{graphicx}
\usepackage{subcaption}

% https://tex.stackexchange.com/a/300259
\usepackage{etoolbox}
\makeatletter
\patchcmd{\ttlh@hang}{\parindent\z@}{\parindent\z@\leavevmode}{}{}
\patchcmd{\ttlh@hang}{\noindent}{}{}{}
\makeatother

\bibliography{thesis}

\newcommand\abs[1]{\left|\hspace{0.6999pt}#1\hspace{0.6999pt}\right|}
\newcommand\set[1]{\left\{#1\right\}}
\newcommand\setdef[2]{\left\{#1\,|\,#2\right\}}
\newcommand\avg[1]{\overline{#1}}
\newcommand\E{\mathbb E}
\newcommand\mean{\mu}
\newcommand\Nat{\mathbb N}
\newcommand\Prob{\mathbb P}
\newcommand\ProbCond[2]{\Prob[#1\,|\,#2]}
\newcommand\Var{\mathbb V}
\DeclareMathOperator\Cov{Cov}
\DeclareMathOperator*{\argmin}{arg\,min}
%\DeclarePairedDelimiter\norm{\lvert}{\rvert}
\newcommand\norm[1]{\left\lvert{#1}\right\rvert}
\newcommand\var{\sigma^2}
\newcommand\sd{\sigma}
\newcommand\transpose{T}
\newcommand\card[1]{\left|\hskip .06667em #1\hskip .06667em \right|}

\newcommand\symrepr[1]{\texttt{#1}}
\newcommand\usymrepr[2]{\texttt{#1} ({#2})}
\newcommand\TODO[1]{\textbf{ToDo:} #1 \par}

\renewcommand\theauthor{Lukas Prokop}
\newcommand\authormail{admin@lukas-prokop.at}
\newcommand\institute{Department of Mathematics}
\newcommand\university{University of Kobe, Japan}

\title{Machine Learning using Bayesian statistics and Neural networks}
\author{\theauthor}
\date{Oct 2016 to 2017}

\definecolor{linkcolor}{cmyk}{0.68,0.00,0.71,0.39}
\definecolor{citecolor}{cmyk}{0.60,0.00,0.75,0.51}
\definecolor{urlcolor}{cmyk}{0.59,0.00,0.76,0.44}

\defaultfontfeatures{Ligatures=TeX}%,Numbers=OldStyle}

\setminted[python]{mathescape,numbersep=5pt,frame=lines,framesep=2mm}

\setromanfont{Andada}
\setsansfont{DejaVu Sans}
\setCJKmainfont{IPAGothic}
\setCJKsansfont{IPAGothic}
\setCJKfallbackfamilyfont{rm}{IPAGothic}

\newfontfamily\deja{DejaVu Sans}

\hypersetup{
  pdfpagemode={UseOutlines},
  pdftitle={Bayesian statistics and Neural networks},
  pdfauthor={Lukas Prokop},
  pdfkeywords={bachelor thesis, Machine learning, Bayesian statistics, probability theory, Neural networks},
  bookmarksopen=true,
  bookmarksopenlevel=0,
  hypertexnames=false,
  colorlinks=true,
  citecolor=citecolor,
  linkcolor=linkcolor,
  urlcolor=urlcolor,
  pdfstartview={FitV},
  unicode,
  breaklinks=true
}

\allowdisplaybreaks

% theorem design via http://tex.stackexchange.com/a/262573
\newlength\exampleindent
\setlength{\exampleindent}{20pt}
\theorembodyfont{\normalfont}
\theoremindent\exampleindent
\theoremrightindent 10pt
\theoremstyle{plain}

%\newtheoremstyle{mytheoremstyle}% name
%    {\topsep}% Space above
%    {\topsep}% Space below
%    {\itshape}% Body font
%    {}% Indent amount
%    {\normalfont}% Theorem head font
%    {.}% Punctuation after theorem head
%    {.5em}% Space after theorem head
%    {}% Theorem head spec (can be left empty, meaning ‘normal’)

%\theoremstyle{mytheoremstyle}

\newtheorem{example}{Example}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proof}{Proof}
\newtheorem{problem}{Problem}

\renewcommand{\baselinestretch}{1.2}

\setlength{\parindent}{0pt}%
\setlength{\parskip}{\baselineskip}%

\makeindex

\begin{document}
\frontmatter
\begin{titlepage}
  \begin{center}
    \textsc{\Large Bachelor Thesis, Mathematics}\\[1cm]

    {\huge \bfseries ``Machine Learning using \\ Bayesian statistics and Neural networks''\\}%
    \vspace{1.7cm}%

    \large \textit{A thesis submitted in fulfillment of the requirements \\ for the bachelor's degree in Mathematics}%
    \vspace{1cm}%

    \begin{minipage}[t]{0.6\textwidth}%
      \centering\large
      \begin{tabular}{rl}
        Author:       & \href{mailto:\authormail}{\theauthor} \\
        Supervisor:   & 高山・信毅 {\deja ※} \\
        Supervisor:   & Bredies Kristian †
      \end{tabular}
    \end{minipage}%
    \vspace{1cm}%

    \begin{minipage}[t]{0.6\textwidth}%
     \centering
      {\deja ※} University of Kobe, Japan \\
      † University of Graz, Austria
    \end{minipage}%
    \vspace{3cm}

    {\large Version: \today}\\[14pt]
    \begin{minipage}{0.4\textwidth}
      \centering
      \includegraphics[width=\textwidth]{img/kobe.pdf}
    \end{minipage}
    \vfill
  \end{center}
\end{titlepage}
\thispagestyle{empty}

\newcommand{\nextoddpage}{
  \if@openright\cleardoublepage\else\clearpage\fi
  \ifdef{\phantomsection}{\phantomsection}{}
}

\newenvironment{declaration}{
  {\nextoddpage}
  %{\space Affidavit}
  {\thispagestyle{plain}}
  {\null\vfil}
  {\noindent%\huge\bfseries{
    \huge{\textsc{Affidavit}}
  %}
  \par\vspace{10pt}}
}{}

\NewDocumentEnvironment{acknowledgements}{}{%
  {\nextoddpage}
  %\tttypeout{\acknowledgementname}
  \thispagestyle{plain}
  \begin{center}{
      %\huge\textit{
      \Large\textsc{Acknowledgements}
      %}
    \par}\end{center}
}{\vfil\vfil\null}

\NewDocumentEnvironment{abstract}{}{%
  {\nextoddpage}
  \thispagestyle{plain}
  \begin{center}{\huge\textsc{Abstract}\par}\end{center}
}{\vfil\vfil\null}

\clearpage
\ifodd\value{page}\else\hbox{}\newpage\fi

\begin{abstract}%
  \parskip5pt
  Machine Learning is a vivid research area. Machine Learning fundamentally changes
  the idea that programmers mechanically write programs in order to perform
  tasks such as classification, regression, clustering, density estimation, and model selection.
  In supervised learning, machines learn by observing test vectors and their desired output. 
  They successively adapt their estimation of the input to return desired outputs for actual data.
  Validation data is used to verify whether this estimate performs good on input,
  the machine has not learned about. Recent efforts such as Google DeepMind's AlphaGo
  or Neural Algorithms of Artistic Style (by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge)
  show the success of Machine Learning in real-world applications.
  This thesis will cover Bayesian and Neural Networks as two branches of Machine Learning.

  Bayesian networks consider probabilities as quantification of belief.
  Bayes' Theorem is used as a very generic tool to establish a relation
  between the prior, a likelihood, and the posterior belief.
  For example, consider a set of random variables modelling symptoms of a disease.
  Probabilistic dependencies between these symptoms exist inherently.
  For a set of sample patients, the belief in experiencing certain symptoms and a certain disease is specified.
  The machine can now learn these input-output relationships.
  In the following for a new patient, the belief in symptoms is provided
  and the machine returns a belief whether the patient suffers from a certain disease.

  Neural networks are another branch of Machine Learning. Neural networks consist of multiple
  layers of neurons. Input signals traverse these layers and using sophisticated algorithms,
  neuron weights are adjusted so that neurons learn which input signals belong to which
  output class. This research area is prominent since the 1970s, when Paul Werbos described
  the Backpropagation algorithm in is Ph.D. thesis.

  This bachelor thesis sums up fundamental theorems and theoretical background of the aforementioned fields.
  Furthermore it covers technical details of my implementation to recognize mathematical expressions.

  \vspace{3pt}
  \textbf{Keywords:}
    Machine learning, Bayesian network, Bayesian statistics, Bayes' Theorem, probability theory, Neural networks
\end{abstract}

\begin{abstract}%
  \parskip5pt
  抽象は日本語で仕上がます… \dots

  \vspace{3pt}
  \textbf{Keywords:}
    Machine learning, Bayesian statistics, Bayes' Theorem, Probability theory, Neural networks
\end{abstract}

\begin{acknowledgements}%
  \parskip5pt
  I would like to thank my advisor 高山先生 of Kobe University for his continued effort during my studies.
  Thank you for the valuable input and one year of academic support.

  I would also like to thank my Austrian advisor Bredies Kristian for a final revision and grading
  at University of Graz.

  During my year abroad in Japan, I met lot of new people and gained valuable experiences.
  But I wouldn't have taken the challenge without Martina. Thank you for staying with me
  and sharing your experiences continuously.

  どうもありがとうございました。
\end{acknowledgements}

All source codes are available at \href{http://lukas-prokop.at/proj/bakk\_kobe}{lukas-prokop.at/proj/bakk\_kobe}
and published under terms and conditions of Free/Libre Open Source Software.
This document was printed with \XeLaTeX{} in the Andada typeface.

\tableofcontents
\mainmatter

\input{bayesian}
\input{nn}
\input{appendix}

\backmatter
\printindex
\printbibliography
%\listoffigures
%\listoftables
%\printindex

\end{document}
